{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %matplotlib inline\n",
    "%matplotlib notebook\n",
    "from IPython.display import Audio, display\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "from shutil import copy, rmtree\n",
    "from os.path import join as jp\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "from scipy.io import wavfile as wf\n",
    "from time import time, sleep\n",
    "from input_data import prepare_words_list\n",
    "from glob import glob\n",
    "import hashlib\n",
    "from classes import get_classes, get_int2label, get_label2int\n",
    "def float_audio(x, sample_rate=16000, autoplay=False):\n",
    "    # avoid Audio normalizing the data\n",
    "    data = np.int16(x * 32768)\n",
    "    fn = '/tmp/%s.wav' % data[:5]\n",
    "    wf.write(fn, sample_rate, data)\n",
    "    display(Audio(filename=fn, rate=sample_rate, autoplay=autoplay))\n",
    "\n",
    "def plot_audio(data, sample_rate=16000, normed=False):\n",
    "    if not normed:\n",
    "        data = np.float32(data) / 32768\n",
    "    plt.figure()\n",
    "    plt.plot(np.arange(sample_rate), data)\n",
    "    plt.axis([0, sample_rate, -1, 1])\n",
    "    \n",
    "def center_pad(data, desired_size=16000):\n",
    "    missing = desired_size - len(data)\n",
    "    pad_left = missing // 2\n",
    "    pad_right = missing - pad_left\n",
    "    padded_data = np.pad(data, (pad_left, pad_right), mode='constant')\n",
    "    return padded_data\n",
    "\n",
    "def random_crop(data, desired_size=16000):\n",
    "    start = np.random.randint(len(data) - desired_size)\n",
    "    return data[start: start + desired_size]\n",
    "\n",
    "def normalized_read(fn, desired_size=16000):\n",
    "    rate, data = wf.read(fn)\n",
    "    data = np.float32(data) / 32768\n",
    "    assert rate == 16000\n",
    "    if len(data) < desired_size:\n",
    "        data = center_pad(data, desired_size=desired_size)\n",
    "    elif len(data) > desired_size:\n",
    "        data = random_crop(data, desired_size=desired_size)\n",
    "    return data\n",
    "\n",
    "def md5(fname):\n",
    "    hash_md5 = hashlib.md5()\n",
    "    with open(fname, \"rb\") as f:\n",
    "        for chunk in iter(lambda: f.read(4096), b\"\"):\n",
    "            hash_md5.update(chunk)\n",
    "    return hash_md5.hexdigest()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub1 = pd.read_csv('submission_033b.csv')  # 87% PLB\n",
    "sub2 = pd.read_csv('submission_017.csv')  # 87% PLB\n",
    "sub3 = pd.read_csv('submission_018.csv')  # 86% PLB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_total = sub1.shape[0]\n",
    "fname_equal = (sub1.fname == sub2.fname).sum()\n",
    "assert fname_equal == num_total\n",
    "equal = sub1.label == sub2.label\n",
    "unequal = ~equal\n",
    "print(\"Neq: \", equal.sum(), \"Nun: \", unequal.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display where classifiers disagree\n",
    "These samples are acutally quite hard!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_DIR = 'data/test/audio'\n",
    "for i in range(200):\n",
    "    if unequal[i]:\n",
    "        fn = os.path.join(TEST_DIR, sub1.loc[i, 'fname'])\n",
    "        print(fn, ': ', sub1.loc[i, 'label'], \" vs \", sub2.loc[i, 'label'])\n",
    "        display(Audio(fn))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show (predicted) test data distribution\n",
    "### Note that during training silence prob was 10%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_bar(submission):\n",
    "    counts = []\n",
    "    for label_name in submission.label.unique():\n",
    "        label_count = (submission.label == label_name).sum()\n",
    "        percent = (label_count / num_total) * 100.0\n",
    "        counts.append((label_name, percent))\n",
    "    print(counts)\n",
    "    plt.bar(range(len(counts)), [c[-1] for c in counts])\n",
    "    _ = plt.xticks(range(len(counts)), [c[0] for c in counts])\n",
    "    plt.grid('on')\n",
    "    plt.xlabel('Label names')\n",
    "    plt.ylabel('Precentage [%]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_bar(sub1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compress and decompress using mu-law"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_wav(fn):\n",
    "    rate, data = wf.read(fn)\n",
    "    assert rate == 16000\n",
    "    data = data / 32768\n",
    "    missing = 16000 - len(data)\n",
    "    data = np.pad(data, (missing, 0), mode='constant')\n",
    "    return data\n",
    "\n",
    "\n",
    "def compress(x, mu=255):\n",
    "    assert x.min() >= -1 and x.max() <= 1\n",
    "    compressed = np.sign(x) * np.log10(1.0 + mu * np.abs(x)) / np.log10(1.0 + mu)\n",
    "    return compressed\n",
    "\n",
    "def decompress(x, mu=255):\n",
    "    assert x.min() >= -1 and x.max() <= 1\n",
    "    decompressed= np.sign(x) * (1.0 / mu) * (np.power(1.0 + x, np.abs(x)) - 1.0)\n",
    "    return decompressed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "up_sample = load_wav('data/test/audio/clip_001204892.wav')\n",
    "float_audio(up_sample)\n",
    "c = compress(up_sample)\n",
    "d = decompress(c)\n",
    "float_audio(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Audio(data=np.int16(up_sample  * 32768), rate=16000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use average pooling to reduce signal size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import AveragePooling1D, Input, Reshape\n",
    "def avg_model(sample_rate=16000):\n",
    "    input_layer = Input([sample_rate])\n",
    "    x = input_layer\n",
    "    x = Reshape([-1, 1])(x)\n",
    "    x = AveragePooling1D(2)(x)\n",
    "    return Model(input_layer, x)\n",
    "\n",
    "model = avg_model()\n",
    "out = model.predict([up_sample.reshape((1, -1))]).squeeze()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.title('Orginal')\n",
    "plt.plot(up_sample)\n",
    "plt.figure()\n",
    "plt.title('Subsamples')\n",
    "plt.plot(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "float_audio(out, sample_rate=len(out))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How is the padding handeled by decode_wav?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.contrib.framework.python.ops import audio_ops as contrib_audio\n",
    "from tensorflow.python.ops import io_ops\n",
    "\n",
    "wav_filename_placeholder = tf.placeholder(tf.string, [])\n",
    "wav_loader = io_ops.read_file(wav_filename_placeholder)\n",
    "wav_decoder = contrib_audio.decode_wav(\n",
    "    wav_loader, desired_channels=1)\n",
    "fns = sorted(glob('data/train/audio/go/*.wav'))\n",
    "short_fn = ''\n",
    "with tf.Session() as sess:\n",
    "    for fn in fns:\n",
    "        out = sess.run(wav_decoder.audio,\n",
    "                       {wav_filename_placeholder: fn_val})\n",
    "        if out.shape[0] != 16000:\n",
    "            short_fn = fn\n",
    "            print(fn)\n",
    "            break\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Audio(short_fn, autoplay=True))\n",
    "sleep(1)\n",
    "float_audio(out, autoplay=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check how many samples are shorter than 1s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "train_fns = sorted(glob('data/train/audio/*/*.wav'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_wav(fn):\n",
    "    rate, data = wf.read(fn)\n",
    "    assert rate == 16000\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "short_count = 0\n",
    "for fn in tqdm(train_fns):\n",
    "    data = load_wav(fn)\n",
    "    if len(data) != 16000:\n",
    "        short_count += 1\n",
    "print(\"Short: \", short_count)\n",
    "print(\"All: \", len(train_fns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize augmented training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division, print_function\n",
    "from keras import backend as K\n",
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
    "from keras.callbacks import TensorBoard\n",
    "from callbacks import ConfusionMatrixCallback\n",
    "from model import speech_model, prepare_model_settings\n",
    "from input_data import AudioProcessor, prepare_words_list\n",
    "from classes import get_classes\n",
    "from IPython import embed  # noqa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_gen(audio_processor, sess,\n",
    "             batch_size=128,\n",
    "             background_frequency=0.5, background_volume_range=0.2,\n",
    "             foreground_frequency=0.5, foreground_volume_range=0.2,\n",
    "             time_shift=(100.0 * 16000.0) / 1000,\n",
    "             mode='validation'):\n",
    "    offset = 0\n",
    "    if mode != 'training':\n",
    "        background_frequency = 0.0\n",
    "        background_volume_range = 0.0\n",
    "        foreground_frequency = 0.0\n",
    "        foreground_volume_range = 0.0\n",
    "        time_shift = 0\n",
    "    while True:\n",
    "        X, y = audio_processor.get_data(\n",
    "            how_many=batch_size, offset=0 if mode == 'training' else offset,\n",
    "            background_frequency=background_frequency,\n",
    "            background_volume_range=background_volume_range,\n",
    "            foreground_frequency=foreground_frequency,\n",
    "            foreground_volume_range=foreground_volume_range,\n",
    "            time_shift=time_shift, mode=mode, sess=sess)\n",
    "        offset += batch_size\n",
    "        if offset > ap.set_size(mode) - batch_size:\n",
    "              offset = 0\n",
    "        yield X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = K.get_session()\n",
    "data_dirs = ['data/train/audio']\n",
    "add_pseudo = True\n",
    "if add_pseudo:\n",
    "    data_dirs.append('data/pseudo/audio')\n",
    "compute_mfcc = False\n",
    "sample_rate = 16000\n",
    "batch_size = 100\n",
    "classes = get_classes(wanted_only=False)\n",
    "model_settings = prepare_model_settings(\n",
    "  label_count=len(prepare_words_list(classes)), sample_rate=sample_rate,\n",
    "  clip_duration_ms=1000, window_size_ms=30.0, window_stride_ms=10.0,\n",
    "  dct_coefficient_count=40)\n",
    "ap = AudioProcessor(\n",
    "  data_dirs=data_dirs,\n",
    "  silence_percentage=10.0,\n",
    "  unknown_percentage=7.0,\n",
    "  wanted_words=classes,\n",
    "  validation_percentage=10.0,\n",
    "  testing_percentage=0.0,\n",
    "  model_settings=model_settings,\n",
    "  compute_mfcc=compute_mfcc)\n",
    "train_gen = data_gen(ap, sess, batch_size=batch_size, mode='training')\n",
    "val_gen = data_gen(ap, sess, batch_size=batch_size, mode='validation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ap.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disp_count = 0\n",
    "while True:\n",
    "    if disp_count > 5:\n",
    "        break\n",
    "    X, y = next(train_gen)\n",
    "    for i in range(X.shape[0]):\n",
    "        if y[i].argmax() == 10:\n",
    "            sample = X[i, :].squeeze()\n",
    "            float_audio(sample, autoplay=True)\n",
    "            sleep(1)\n",
    "            disp_count += 1\n",
    "            # plt.figure()\n",
    "            # plt.axis([0, 16000, -1, 1])\n",
    "            # plt.plot(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create pseudo labels from consistent predictions\n",
    "### [old pseudo]: Sub1: 84% PLB, Sub2: 84% PLB, Sub3: 82% PLB\n",
    "### Sub1: 87% PLB, Sub2: 87% PLB, Sub3: 86% PLB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub1 = pd.read_csv('submission_033b_all_labels.csv')  # 87% PLB\n",
    "sub2 = pd.read_csv('submission_017_all_labels.csv')  # 87% PLB\n",
    "sub3 = pd.read_csv('submission_018_all_labels.csv')  # 86% PLB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All:  158538  consistend:  132222\n"
     ]
    }
   ],
   "source": [
    "consistend = ((sub1.label == sub2.label) & (sub1.label == sub3.label))\n",
    "print(\"All: \", sub1.shape[0], \" consistend: \", consistend.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 158538/158538 [01:29<00:00, 1766.18it/s]\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm(range(sub1.shape[0])):\n",
    "    fn = sub1.loc[i, 'fname']\n",
    "    if fn != sub2.loc[i, 'fname'] or fn != sub3.loc[i, 'fname']:\n",
    "        print(\"Fatal error\")\n",
    "        break\n",
    "    if consistend[i]:\n",
    "        label = sub1.loc[i, 'label']\n",
    "        dst_fn = jp('data', 'pseudo', 'audio', label, fn)\n",
    "        src_fn = jp('data', 'test', 'audio', fn)\n",
    "        copy(src_fn, dst_fn, follow_symlinks=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Listen to submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = get_classes(wanted_only=False, extend_reversed=True)\n",
    "[w for w in words if 'b' in w]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = pd.read_csv('submission_034_all_labels.csv')\n",
    "label2int = get_label2int(wanted_only=True)\n",
    "disp_count = 0\n",
    "for i in reversed(range(sub.shape[0])):\n",
    "    if disp_count >= 3:\n",
    "        break\n",
    "    fn = jp('data', 'test', 'audio', sub.loc[i, 'fname'])\n",
    "    label = sub.loc[i, 'label']\n",
    "    if label == 'go' and np.random.rand() > 0.5:\n",
    "        print(label)\n",
    "        display(Audio(fn, autoplay=True))\n",
    "        sleep(1)\n",
    "        disp_count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "?float_audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "np.random.seed(5)\n",
    "sub = pd.read_csv('submission_034_all_labels.csv')\n",
    "label2int = get_label2int(wanted_only=True)\n",
    "disp_count = 0\n",
    "for i in reversed(range(sub.shape[0])):\n",
    "    if disp_count >= 2:\n",
    "        break\n",
    "    label = sub.loc[i, 'label']\n",
    "    if label == 'silence' and np.random.rand() > 0.999:\n",
    "        fn = jp('data', 'test', 'audio', sub.loc[i, 'fname'])\n",
    "        rate, data = wf.read(fn)\n",
    "        data = np.float32(data) / 32767\n",
    "        # data = np.sqrt(data) + 0.5\n",
    "        data = 10 ** (data) - 1.0\n",
    "        plot_audio(data, normed=True)\n",
    "        print(fn, label)\n",
    "        float_audio(data, autoplay=True)\n",
    "        sleep(1)\n",
    "        disp_count += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Listen to submission with probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Wanted: \", get_classes(wanted_only=True))\n",
    "print(\"All: \", get_classes(wanted_only=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sub = pd.read_csv('prob_ensemble_002_all_labels_probs.csv')\n",
    "label2int = get_label2int(wanted_only=False)\n",
    "disp_count = 0\n",
    "for i in reversed(range(sub.shape[0])):\n",
    "    if disp_count >= 5:\n",
    "        break\n",
    "    fn = jp('data', 'test', 'audio', sub.loc[i, 'fname'])\n",
    "    label = sub.loc[i, 'label']\n",
    "    if label == 'unknown' and np.random.rand() > 0.99:\n",
    "        print(label)\n",
    "        probs = sub.loc[i, label2int.keys()]\n",
    "        plt.figure()\n",
    "        plt.title(str(disp_count))\n",
    "        plt.bar(range(len(probs)), probs)\n",
    "        plt.axis([0, len(probs), 0, 1])\n",
    "        plt.xticks(range(len(probs)), label2int.keys(), rotation='vertical')\n",
    "        display(Audio(fn, autoplay=True))\n",
    "        sleep(1)\n",
    "        disp_count += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correct broken submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_broken = pd.read_csv('submission_012.csv')\n",
    "sub_all = pd.read_csv('submission_012_all_labels.csv')\n",
    "sub_broken.loc[(sub_all.label == 'unknown').values, 'label'] = 'silence'\n",
    "sub_broken.to_csv('submission_012_corrected.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = pd.read_csv('submission_012_corrected.csv')\n",
    "disp_count = 0\n",
    "for i in range(sub.shape[0]):\n",
    "    if disp_count >= 10:\n",
    "        break\n",
    "    fn = jp('data', 'test', 'audio', sub.loc[i, 'fname'])\n",
    "    label = sub.loc[i, 'label']\n",
    "    if label != 'unknown' and np.random.rand() > 0.9:\n",
    "        print(label)\n",
    "        display(Audio(fn, autoplay=True))\n",
    "        sleep(1)\n",
    "        disp_count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = pd.read_csv('submission_012_all_labels.csv')\n",
    "counts = sub.label.value_counts()\n",
    "percent_counts = counts.apply(lambda x: np.round(100.0 * (x / sub.shape[0])))\n",
    "# print(counts)\n",
    "print(percent_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check what the generators are producing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
    "from keras.callbacks import TensorBoard\n",
    "from model import speech_model, prepare_model_settings\n",
    "from input_data import AudioProcessor, prepare_words_list\n",
    "from classes import get_classes\n",
    "\n",
    "\n",
    "def data_gen(audio_processor, sess,\n",
    "             batch_size=128,\n",
    "             background_frequency=0.5, background_volume_range=0.2,\n",
    "             foreground_frequency=1.0, foreground_volume_range=3.0,\n",
    "             time_shift=(100.0 * 16000.0) / 1000,\n",
    "             mode='validation'):\n",
    "    offset = 0\n",
    "    if mode != 'training':\n",
    "        background_frequency = 0.0\n",
    "        background_volume_range = 0.0\n",
    "        foreground_frequency = 0.0\n",
    "        foreground_volume_range = 0.0\n",
    "        time_shift = 0\n",
    "\n",
    "    while True:\n",
    "        X, y = audio_processor.get_data(\n",
    "            how_many=batch_size, offset=0 if mode == 'training' else offset,\n",
    "            background_frequency=background_frequency,\n",
    "            background_volume_range=background_volume_range,\n",
    "            foreground_frequency=foreground_frequency,\n",
    "            foreground_volume_range=foreground_volume_range,\n",
    "            time_shift=time_shift, mode=mode, sess=sess)\n",
    "        offset += batch_size\n",
    "        if offset > ap.set_size(mode) - batch_size:\n",
    "            offset = 0\n",
    "        yield X, y\n",
    "\n",
    "\n",
    "sess = K.get_session()\n",
    "data_dirs = ['data/train/audio']\n",
    "add_pseudo = False\n",
    "if add_pseudo:\n",
    "    data_dirs.append('data/pseudo/audio')\n",
    "compute_mfcc = False\n",
    "sample_rate = 16000\n",
    "batch_size = 64\n",
    "classes = get_classes(wanted_only=False)\n",
    "model_settings = prepare_model_settings(\n",
    "  label_count=len(prepare_words_list(classes)), sample_rate=sample_rate,\n",
    "  clip_duration_ms=1000, window_size_ms=30.0, window_stride_ms=10.0,\n",
    "  dct_coefficient_count=40)\n",
    "ap = AudioProcessor(\n",
    "  data_dirs=data_dirs,\n",
    "  silence_percentage=15.0,\n",
    "  unknown_percentage=7.0,\n",
    "  wanted_words=classes,\n",
    "  validation_percentage=10.0,\n",
    "  testing_percentage=0.0,\n",
    "  model_settings=model_settings,\n",
    "  compute_mfcc=compute_mfcc)\n",
    "train_gen = data_gen(ap, sess, batch_size=batch_size, mode='training')\n",
    "val_gen = data_gen(ap, sess, batch_size=batch_size, mode='validation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ap.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wanted_words = get_classes(wanted_only=True)\n",
    "print(wanted_words)\n",
    "labels2int = {v: k for k, v in ap.word_to_index.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf data/train/audio/_silence_\n",
    "!mkdir data/train/audio/_silence_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# save silence samples to '_silence_'\n",
    "disp_count = 0\n",
    "for i in range(100):\n",
    "    print(disp_count)\n",
    "    if disp_count > 100:\n",
    "        break\n",
    "    X, y = next(train_gen)\n",
    "    y = y.argmax(axis=-1)\n",
    "    for j in range(len(y)):\n",
    "        if y[j] == 0:\n",
    "            print(labels2int[y[j]])\n",
    "            out_fn = jp('data/train/audio/_silence_/%05d.wav' % disp_count)\n",
    "            data = np.int16(X[j, :] * 32768)\n",
    "            wf.write(out_fn, 16000, data)\n",
    "            # float_audio(X[j, :], autoplay=True)\n",
    "            # sleep(1)\n",
    "            disp_count += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load model and check where we go wrong in the validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = speech_model(\n",
    "  'conv_1d_time',\n",
    "  model_settings['fingerprint_size'] if compute_mfcc else sample_rate,\n",
    "  num_classes=model_settings['label_count'])\n",
    "model.load_weights('checkpoints_014/ep-039-loss-0.173.hdf5')\n",
    "disp_count = 0\n",
    "for i in range(ap.set_size('validation') // batch_size):\n",
    "    if disp_count >= 10:\n",
    "        break\n",
    "    X, y_true = next(val_gen)\n",
    "    y_true = y_true.argmax(axis=-1)\n",
    "    y_pred = model.predict(X).argmax(axis=-1)\n",
    "    for j in range(len(y_true)):\n",
    "        if y_true[j] != y_pred[j] and np.random.rand() > 0.5:\n",
    "            l_true = labels2int[y_true[j]]\n",
    "            l_pred = labels2int[y_pred[j]]\n",
    "            print(\"Pred: %s, Actual: %s\" % (l_pred, l_true))\n",
    "            float_audio(X[j, :], autoplay=True)\n",
    "            sleep(1)\n",
    "            disp_count += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create \"unknown\" words\n",
    "### Just reverse some of the unwanted words. Reversing the wanted words might actually cause some problems when using global pooling in the end?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(exclude_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wanted_words = get_classes(wanted_only=True)\n",
    "# wow reversed is wow! one reversed sounds like no!\n",
    "# dog vs go :P\n",
    "exclude_words = wanted_words + ['_background_noise_', 'wow', 'one', 'dog']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fns = sorted(glob(jp('data', 'train', 'audio', '*', '*.wav')))\n",
    "print(len(fns))\n",
    "fns = [fn for fn in fns if fn.split('/')[-2] not in exclude_words]\n",
    "print(len(fns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_classes = list(set([fn.split('/')[-2] for fn in fns]))\n",
    "# new_classes.remove('_silence_')\n",
    "new_classes = ['new_' + new_class[::-1] for new_class in new_classes]\n",
    "print(new_classes)\n",
    "for new_class in new_classes:\n",
    "    new_dir = jp('data/train/audio', new_class)\n",
    "    if os.path.exists(new_dir):\n",
    "        rmtree(new_dir)\n",
    "    os.mkdir(new_dir)\n",
    "    print(\"created %s\" % new_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.shuffle(fns)\n",
    "# fn_selection = np.random.choice(fns, size=5000, replace=False)\n",
    "fn_selection = fns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# there (often) 5 times the same speaker said the same word\n",
    "# \n",
    "# ./eight/5fadb538_nohash_4.wav\n",
    "# ./eight/5fadb538_nohash_3.wav\n",
    "# ./eight/5fadb538_nohash_2.wav\n",
    "# ./eight/5fadb538_nohash_1.wav\n",
    "# ./eight/5fadb538_nohash_0.wav\n",
    "\n",
    "counter = {cn: 0 for cn in new_classes}\n",
    "for fn in tqdm(fn_selection):\n",
    "    cn = 'new_' + fn.split('/')[-2][::-1]\n",
    "    sample_rate, data = wf.read(fn)\n",
    "    bn = os.path.basename(fn)\n",
    "    dst = jp('data', 'train', 'audio', cn,\n",
    "             \"%06d.wav\" % counter[cn])\n",
    "    counter[cn] += 1\n",
    "    data = data[::-1]\n",
    "    wf.write(dst, sample_rate, data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Take a look at the noise files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "background_files = sorted(glob(jp('data', 'train', 'audio', '_background_noise_', '*.wav')))\n",
    "background_volumne = 0.8\n",
    "for fn in background_files[5:6]:\n",
    "    print(fn)\n",
    "    rate, data = wf.read(fn)\n",
    "    if len(data) > 16000:\n",
    "        data = data[:16000]\n",
    "    data = np.float32(data) / 32768\n",
    "    data *= background_volumne\n",
    "    print(data.max())\n",
    "    wf.write('tmp.wav', 16000, np.int16(data * 32767))\n",
    "    plot_audio(data, normed=True)\n",
    "    display(Audio('tmp.wav', autoplay=True))\n",
    "    sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "?wf.write"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Take a look at \"happy\" files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "background_files = sorted(glob(jp('data', 'train', 'audio', 'happy', '*.wav')))\n",
    "volumne = 10**1\n",
    "for fn in background_files[5:6]:\n",
    "    print(fn)\n",
    "    rate, data = wf.read(fn)\n",
    "    if len(data) > 16000:\n",
    "        data = data[:16000]\n",
    "    data = np.float32(data) / 32768\n",
    "    data *= volumne\n",
    "    data = np.clip(data, -1, 1)\n",
    "    print(data.max())\n",
    "    wf.write('tmp.wav', 16000, np.int16(data * 32767))\n",
    "    plot_audio(data, normed=True)\n",
    "    display(Audio('tmp.wav', autoplay=True))\n",
    "    sleep(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Majority vote submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from os.path import join as jp\n",
    "from shutil import copy\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "sub_fns = ['submission_011.csv', 'submission_017.csv',\n",
    "           'submission_018.csv', 'submission_014.csv',\n",
    "           'submission_020.csv']\n",
    "subs = [pd.read_csv(sub_fn) for sub_fn in sub_fns]\n",
    "\n",
    "fname, label = [], []\n",
    "clear_majority = 0\n",
    "for i in tqdm(range(subs[0].shape[0])):\n",
    "    fname.append(subs[0].loc[i, 'fname'])\n",
    "    label_counts = {}\n",
    "    for sub in subs:\n",
    "        ll = sub.loc[i, 'label']\n",
    "    if ll in label_counts:\n",
    "        label_counts[ll] += 1\n",
    "    else:\n",
    "        label_counts[ll] = 1\n",
    "\n",
    "    maj_label = max(label_counts, key=label_counts.get)\n",
    "    if label_counts[maj_label] > 2:\n",
    "        clear_majority += 1\n",
    "    else:\n",
    "        # in trouble save the wav files!\n",
    "        src = jp('data', 'test', 'audio', fname[-1])\n",
    "        dst = jp('split_decision', str(label_counts) + fname[-1])\n",
    "        copy(src, dst)\n",
    "    # resolve tie by chosing 'unknown' or 'silence' if available\n",
    "    if 'unknown' in label_counts and 'silence' in label_counts:\n",
    "        maj_label = 'silence'\n",
    "    elif 'unknown' in label_counts:\n",
    "        maj_label = 'unknown'\n",
    "    elif 'silence' in label_counts:\n",
    "        maj_label = 'silence'\n",
    "    label.append(maj_label)\n",
    "\n",
    "pd.DataFrame({'fname': fname, 'label': label}).to_csv(\n",
    "    'majority_sub_011.csv', index=False)\n",
    "print(\"Done! Got a clear majority for %d of %d samples.\"\n",
    "      % (clear_majority, subs[0].shape[0]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate more background noise\n",
    "## The idea is to take the reversed words and average some of them up to generate ~30s tracks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unwanted words reversed\n",
    "fns = sorted(glob(jp('data', 'train', 'audio', 'unwrev', '*.wav')))\n",
    "print(len(fns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "louder = 16.0  # make it louder!\n",
    "sample_secs = 30\n",
    "total_new = 0\n",
    "num_noise_tracks = 400\n",
    "noise_track = []\n",
    "noise_track_counter = 0\n",
    "sample_track = []\n",
    "sample_counter = 0\n",
    "for fn in tqdm(fns):\n",
    "    sample_rate, data = wf.read(fn)\n",
    "    data = np.float32(data) / 32768\n",
    "    assert sample_rate == 16000\n",
    "    data = center_pad(data)\n",
    "    shift = np.random.randint(16000)\n",
    "    data = np.roll(data, shift)\n",
    "    sample_track.append(data)\n",
    "    sample_counter += 1\n",
    "    if sample_counter == sample_secs:\n",
    "        noise_track.append(np.concatenate(sample_track))\n",
    "        sample_track = []\n",
    "        sample_counter = 0\n",
    "        noise_track_counter += 1\n",
    "        if noise_track_counter == num_noise_tracks:\n",
    "            noise_track = np.array(noise_track)\n",
    "            noise_track = np.mean(noise_track, axis=0) * louder\n",
    "            noise_track = np.int16(noise_track * 32768)\n",
    "            out_fn = jp('data', 'train', 'audio', '_background_noise_', 'silence_please_%04d.wav' % total_new)\n",
    "            wf.write(out_fn, 16000, noise_track)\n",
    "            print(out_fn)\n",
    "            display(Audio(out_fn))\n",
    "            noise_track = []\n",
    "            noise_track_counter = 0\n",
    "            total_new += 1\n",
    "            if total_new == 2:\n",
    "                break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How does foreground with mixed background sound?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "background_noise_fns = sorted(glob('data/train/audio/_background_noise_/*.wav'))\n",
    "idx = -3\n",
    "print(background_noise_fns[idx])\n",
    "display(Audio(background_noise_fns[idx], autoplay=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "foreground_fns = sorted(glob('data/train/audio/go/*.wav'))\n",
    "for i in range(3):\n",
    "    foreground_fn = np.random.choice(foreground_fns)\n",
    "    foreground = normalized_read(foreground_fn)\n",
    "    background_fn = np.random.choice(background_noise_fns)\n",
    "    # background_fn = background_noise_fns[-3]\n",
    "    print(background_fn)\n",
    "    background = normalized_read(background_fn)\n",
    "    mixed = np.random.uniform(0.8, 1.2) * foreground + 0.3 * background  # np.random.uniform(0, 2)\n",
    "    mixed = np.clip(mixed, -1, 1)\n",
    "    float_audio(mixed, autoplay=True)\n",
    "    plot_audio(mixed, normed=True)\n",
    "    sleep(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How do the models perform on the validation set?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
    "from keras.callbacks import TensorBoard\n",
    "from model import speech_model, prepare_model_settings\n",
    "from input_data import AudioProcessor, prepare_words_list\n",
    "from keras.models import load_model\n",
    "from classes import get_classes\n",
    "\n",
    "\n",
    "def data_gen(audio_processor, sess,\n",
    "             batch_size=128,\n",
    "             background_frequency=0.5, background_volume_range=0.2,\n",
    "             foreground_frequency=1.0, foreground_volume_range=3.0,\n",
    "             time_shift=(100.0 * 16000.0) / 1000,\n",
    "             mode='validation'):\n",
    "    offset = 0\n",
    "    if mode != 'training':\n",
    "        background_frequency = 0.0\n",
    "        background_volume_range = 0.0\n",
    "        foreground_frequency = 0.0\n",
    "        foreground_volume_range = 0.0\n",
    "        time_shift = 0\n",
    "\n",
    "    while True:\n",
    "        X, y = audio_processor.get_data(\n",
    "            how_many=batch_size, offset=0 if mode == 'training' else offset,\n",
    "            background_frequency=background_frequency,\n",
    "            background_volume_range=background_volume_range,\n",
    "            foreground_frequency=foreground_frequency,\n",
    "            foreground_volume_range=foreground_volume_range,\n",
    "            time_shift=time_shift, mode=mode, sess=sess)\n",
    "        offset += batch_size\n",
    "        if offset > ap.set_size(mode) - batch_size:\n",
    "            offset = 0\n",
    "        yield X, y\n",
    "\n",
    "\n",
    "sess = K.get_session()\n",
    "data_dirs = ['data/train/audio']\n",
    "add_pseudo = False\n",
    "if add_pseudo:\n",
    "    data_dirs.append('data/pseudo/audio')\n",
    "compute_mfcc = False\n",
    "sample_rate = 16000\n",
    "batch_size = 64\n",
    "classes = get_classes(wanted_only=False)\n",
    "model_settings = prepare_model_settings(\n",
    "  label_count=len(prepare_words_list(classes)), sample_rate=sample_rate,\n",
    "  clip_duration_ms=1000, window_size_ms=30.0, window_stride_ms=10.0,\n",
    "  dct_coefficient_count=40)\n",
    "ap = AudioProcessor(\n",
    "  data_dirs=data_dirs,\n",
    "  silence_percentage=15.0,\n",
    "  unknown_percentage=7.0,\n",
    "  wanted_words=classes,\n",
    "  validation_percentage=10.0,\n",
    "  testing_percentage=0.0,\n",
    "  model_settings=model_settings,\n",
    "  compute_mfcc=compute_mfcc)\n",
    "train_gen = data_gen(ap, sess, batch_size=batch_size, mode='training')\n",
    "val_gen = data_gen(ap, sess, batch_size=batch_size, mode='validation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_fns = [\n",
    "    'checkpoints_019/ep-022-vl-0.2916.hdf5',\n",
    "    'checkpoints_018/ep-049-vl-0.2185.hdf5',\n",
    "    'checkpoints_017/ep-036-vl-0.1969.hdf5'\n",
    "]\n",
    "\n",
    "models = []\n",
    "for model_fn in model_fns:\n",
    "    model = load_model(model_fn)\n",
    "    models.append((model_fn, model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate on validation set\n",
    "for model in models:\n",
    "    out = model.evaluate_generator(\n",
    "        val_gen, steps=ap.set_size('validation') // batch_size)\n",
    "    print(model_fn, out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict samples with models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(ap.words_list), len(models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = next(val_gen)\n",
    "sample_idx = 10\n",
    "float_audio(X[sample_idx, :], autoplay=True)\n",
    "out_probs = []\n",
    "for model_fn, model in models:\n",
    "    y_preds = model.predict(X)\n",
    "    out_probs.append((model_fn, y_preds[sample_idx]))\n",
    "\n",
    "for fn, pred in out_probs:\n",
    "    plt.figure()\n",
    "    plt.title(fn)\n",
    "    plt.bar(range(len(pred)), pred)\n",
    "    plt.xticks(range(len(pred)), ap.words_list, rotation='vertical')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wrong ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_plots = 2\n",
    "plot_c = 0\n",
    "model = models[-1][-1]\n",
    "while plot_c < max_plots:\n",
    "    X, y = next(val_gen)\n",
    "    y_preds = model.predict(X)\n",
    "    for i, y_pred in enumerate(y_preds):\n",
    "        if plot_c >= max_plots:\n",
    "            break\n",
    "        if y_pred.argmax() != y[i].argmax():\n",
    "            float_audio(X[i, :], autoplay=True)\n",
    "            sleep(1)\n",
    "            plt.figure()\n",
    "            plt.title(\"Pred: %s, Actual: %s\"\n",
    "                      % (ap.words_list[y_pred.argmax()], ap.words_list[y[i].argmax()]))\n",
    "            plt.bar(range(len(y_pred)), y_pred)\n",
    "            plt.xticks(range(len(y_pred)), ap.words_list, rotation='vertical')\n",
    "            plot_c += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What happens if we just feed plain silence e.g. all 0?\n",
    "### Looks good!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.zeros((2, 16000), dtype=np.float32)\n",
    "pred = model.predict(X)\n",
    "float_audio(X[0, :], autoplay=True)\n",
    "sleep(1)\n",
    "plt.figure()\n",
    "plt.title(\"Silence pred\")\n",
    "plt.bar(range(len(pred[0])), pred[0])\n",
    "_ = plt.xticks(range(len(pred[0])), ap.words_list, rotation='vertical')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How about the noise files?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model(model_fns[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_fns = glob(jp('data/train/audio/_background_noise_/*.wav'))\n",
    "X = []\n",
    "for noise_fn in noise_fns:\n",
    "    X.append(normalized_read(noise_fn))\n",
    "X = np.float32(X)\n",
    "# change volumne\n",
    "X *= 0.2\n",
    "y_pred = model.predict(X)\n",
    "for i in range(X.shape[0]):\n",
    "    # float_audio(X[0, :], autoplay=True)\n",
    "    # sleep(1)\n",
    "    plt.figure()\n",
    "    plt.title(\"Noise pred: %s\" % noise_fns[i])\n",
    "    plt.bar(range(len(y_pred[i])), y_pred[i])\n",
    "    _ = plt.xticks(range(len(y_pred[i])), ap.words_list, rotation='vertical')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission with averaged probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_fns = [\n",
    "    'submission_021_all_labels_probs.csv',\n",
    "    'submission_020_all_labels_probs.csv',\n",
    "    'submission_017b_all_labels_probs.csv',\n",
    "    'submission_026_all_labels_probs.csv'\n",
    "]\n",
    "\n",
    "\n",
    "subs = []\n",
    "for sub_fn in sub_fns:\n",
    "    subs.append(pd.read_csv(sub_fn))\n",
    "\n",
    "wanted_words = prepare_words_list(get_classes(wanted_only=True))\n",
    "int2label = get_int2label(wanted_only=False)\n",
    "label2int = get_label2int(wanted_only=False)\n",
    "\n",
    "avg = 0.0\n",
    "for sub in subs:\n",
    "    avg += sub.loc[:, label2int.keys()].values / len(subs)\n",
    "\n",
    "print(avg.sum(axis=1)[10:20])\n",
    "probabilities = avg\n",
    "print(probabilities.shape)\n",
    "preds = probabilities.argmax(axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_label = []\n",
    "ensemble_sub = subs[0].copy()\n",
    "for i in tqdm(range(ensemble_sub.shape[0])):\n",
    "    label = int2label[preds[i]]\n",
    "    label = label if label != '_silence_' else 'silence'\n",
    "    label = label if label != '_unknown_' else 'unknown'\n",
    "    # label = label if label == 'silence' or label in wanted_words else 'unknown'\n",
    "    ensemble_label.append(label)\n",
    "\n",
    "ensemble_sub['label'] = ensemble_label\n",
    "ensemble_sub[['fname', 'label']].to_csv('prob_ensemble_010_all_labels.csv', index=False)\n",
    "ensemble_sub.loc[:, label2int.keys()] = probabilities\n",
    "ensemble_sub.to_csv('prob_ensemble_010_all_labels_probs.csv', index=False)\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_label = []\n",
    "ensemble_sub = subs[0].copy()\n",
    "small_prob_counter = 0\n",
    "prob_thresh = 0.3\n",
    "for i in tqdm(range(ensemble_sub.shape[0])):\n",
    "    label = int2label[preds[i]]\n",
    "    label = label if label != '_silence_' else 'silence'\n",
    "    label = label if label != '_unknown_' else 'unknown'\n",
    "    label = label if label == 'silence' or label in wanted_words else 'unknown'\n",
    "    # handle \"small\" probs\n",
    "    max_prob = probabilities[i, preds[i]]\n",
    "    if max_prob < prob_thresh:\n",
    "        small_prob_counter += 1\n",
    "        src_fn = jp('data/test/audio', ensemble_sub.loc[i, 'fname'])\n",
    "        dst_fn = jp('split_decision', '%s_%.3f.wav'\n",
    "                    % (label, max_prob))\n",
    "        copy(src_fn, dst_fn)\n",
    "        # map small prob words (not silence) to unknown\n",
    "        label = label if label == 'unknown' else 'silence'\n",
    "    ensemble_label.append(label)\n",
    "\n",
    "\n",
    "print(\"%d of %d with small prob\" % (small_prob_counter, ensemble_sub.shape[0]))\n",
    "ensemble_sub['label'] = ensemble_label\n",
    "ensemble_sub[['fname', 'label']].to_csv(\n",
    "    'prob_ensemble_010_thresh%.2f.csv' % prob_thresh, index=False)\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find duplicates\n",
    "## https://www.kaggle.com/c/tensorflow-speech-recognition-challenge/discussion/44687"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_fns = sorted(glob('data/train/audio/*/*.wav'))\n",
    "test_fns = sorted(glob('data/test/audio/*.wav'))\n",
    "all_fns = train_fns + test_fns\n",
    "print(\"%d train, %d test, %d total\"\n",
    "      % (len(train_fns), len(test_fns), len(all_fns)))\n",
    "all_fns = all_fns[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('md5sums.p'):\n",
    "    md5sums = {}\n",
    "    for fn in tqdm(all_fns):\n",
    "        if fn in md5sums:\n",
    "            print(\"Double fn!\", fn)\n",
    "        checksum = md5(fn)\n",
    "        md5sums[fn] = checksum\n",
    "    # cache\n",
    "    data = {'md5sums': md5sums}\n",
    "    with open('md5sums.p', 'wb') as f:\n",
    "        pickle.dump(data, f)\n",
    "\n",
    "with open('md5sums.p', 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "    md5sums = data['md5sums']\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = {}\n",
    "checksum_fn = {v: k for k, v in md5sums.items()}\n",
    "for fn, checksum in md5sums.items():\n",
    "    if checksum in counts:\n",
    "        counts[checksum] += 1\n",
    "    else:\n",
    "        counts[checksum] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn_counts = []\n",
    "check_to_fns = {}\n",
    "for checksum, count in counts.items():\n",
    "    fn = checksum_fn[checksum]\n",
    "    fn_counts.append((fn, count))\n",
    "    if checksum in check_to_fns:\n",
    "        if fn not in check_to_fns[fn]:\n",
    "            check_to_fns[fn].append(fn)\n",
    "        else:\n",
    "            check_to_fns[fn] = [fn]\n",
    "fn_counts = sorted(fn_counts, key=lambda x: x[-1], reverse=True)\n",
    "print(fn_counts[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [(k, v) for k, v in counts.items() if v == 3710][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mist_fns = [fn for fn, s in md5sums.items() if s == a[0]]\n",
    "bns = [os.path.basename(fn) for fn in mist_fns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are all silence -.- check that we got it right anyway ...\n",
    "some_sub = pd.read_csv('submission_017.csv')\n",
    "for i in range(some_sub.shape[0]):\n",
    "    fn = some_sub.loc[i, 'fname']\n",
    "    if fn in bns:\n",
    "        label = some_sub.loc[i, 'label']\n",
    "        if label != 'silence':\n",
    "            print(label)\n",
    "# check! That was useless -.-"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Listen test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_fns = sorted(glob('data/test/audio/*.wav'))\n",
    "print(len(test_fns))\n",
    "disp_count = 0\n",
    "for test_fn in test_fns:\n",
    "    if disp_count > 10:\n",
    "        break\n",
    "    if np.random.rand() > 0.7:\n",
    "        print(test_fn)\n",
    "        display(Audio(filename=test_fn,\n",
    "                      rate=16000,\n",
    "                      autoplay=True))\n",
    "        sleep(1)\n",
    "        disp_count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
