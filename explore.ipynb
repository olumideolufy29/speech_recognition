{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "# %matplotlib notebook\n",
    "from IPython.display import Audio, display\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import librosa as lr\n",
    "import numpy as np\n",
    "import os\n",
    "from shutil import copy, rmtree\n",
    "from os.path import join as jp\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "from scipy.io import wavfile as wf\n",
    "from time import time, sleep\n",
    "from input_data import prepare_words_list\n",
    "from glob import glob\n",
    "import hashlib\n",
    "from classes import get_classes, get_int2label, get_label2int\n",
    "from keras.layers import Input, Lambda\n",
    "from model import prepare_model_settings, relu6, overlapping_time_slice_stack\n",
    "from keras.applications.mobilenet import DepthwiseConv2D\n",
    "from keras.models import Model\n",
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.framework.python.ops import audio_ops as contrib_audio\n",
    "from tensorflow.python.ops import io_ops\n",
    "from keras.layers import Input, Conv1D, Reshape\n",
    "from keras.models import Model, load_model\n",
    "\n",
    "def my_load_model(fn):\n",
    "    return load_model(fn, {'relu6': relu6, 'DepthwiseConv2D': DepthwiseConv2D,\n",
    "                           'overlapping_time_slice_stack': overlapping_time_slice_stack})\n",
    "\n",
    "def pad_crop(data, desired_size=16000):\n",
    "    data_len = len(data)\n",
    "    if data_len < desired_size:\n",
    "        missing = desired_size - data_len\n",
    "        data = np.pad(data, (missing, 0), mode='constant')\n",
    "    else:\n",
    "        data = data[:desired_size]\n",
    "    return data\n",
    "\n",
    "def float_audio(x, sample_rate=16000, autoplay=False):\n",
    "    # avoid Audio normalizing the data\n",
    "    data = np.int16(x * 32768)\n",
    "    fn = '/tmp/%s.wav' % data[:5]\n",
    "    wf.write(fn, sample_rate, data)\n",
    "    display(Audio(filename=fn, rate=sample_rate, autoplay=autoplay))\n",
    "\n",
    "def plot_audio(data, sample_rate=16000, normed=False):\n",
    "    if not normed:\n",
    "        data = np.float32(data) / 32768\n",
    "    plt.figure()\n",
    "    plt.plot(np.arange(sample_rate), data)\n",
    "    plt.axis([0, sample_rate, -1, 1])\n",
    "    \n",
    "def center_pad(data, desired_size=16000):\n",
    "    missing = desired_size - len(data)\n",
    "    pad_left = missing // 2\n",
    "    pad_right = missing - pad_left\n",
    "    padded_data = np.pad(data, (pad_left, pad_right), mode='constant')\n",
    "    return padded_data\n",
    "\n",
    "def random_crop(data, desired_size=16000):\n",
    "    start = np.random.randint(len(data) - desired_size)\n",
    "    return data[start: start + desired_size]\n",
    "\n",
    "def normalized_read(fn, desired_size=16000):\n",
    "    rate, data = wf.read(fn)\n",
    "    data = np.float32(data) / 32768\n",
    "    assert rate == 16000\n",
    "    if len(data) < desired_size:\n",
    "        data = center_pad(data, desired_size=desired_size)\n",
    "    elif len(data) > desired_size:\n",
    "        data = random_crop(data, desired_size=desired_size)\n",
    "    return data\n",
    "\n",
    "def md5(fname):\n",
    "    hash_md5 = hashlib.md5()\n",
    "    with open(fname, \"rb\") as f:\n",
    "        for chunk in iter(lambda: f.read(4096), b\"\"):\n",
    "            hash_md5.update(chunk)\n",
    "    return hash_md5.hexdigest()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "sub1 = pd.read_csv('submission_033b.csv')  # 87% PLB\n",
    "sub2 = pd.read_csv('submission_017.csv')  # 87% PLB\n",
    "sub3 = pd.read_csv('submission_018.csv')  # 86% PLB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check where all three disagree\n",
    "unequal = ((sub1.label != sub2.label) & (sub1.label != sub3.label))\n",
    "print(\"%d of %d are unequal\" % (unequal.sum(), sub1.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display where classifiers disagree\n",
    "These samples are acutally quite hard!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_DIR = 'data/test/audio'\n",
    "disp_count = 0\n",
    "for i in range(len(unequal)):\n",
    "    if disp_count > 5:\n",
    "        break\n",
    "    if unequal[i] and np.random.rand() > 0.99:\n",
    "        fn = os.path.join(TEST_DIR, sub1.loc[i, 'fname'])\n",
    "        print(fn, ': ', sub1.loc[i, 'label'], \" vs \", sub2.loc[i, 'label'])\n",
    "        display(Audio(fn, autoplay=True))\n",
    "        sleep(1)\n",
    "        disp_count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save these \"hard\" ones\n",
    "for i in tqdm(range(len(unequal))):\n",
    "    if unequal[i]:\n",
    "        bn = sub1.loc[i, 'fname']\n",
    "        src_fn = os.path.join('data/test/audio', bn)\n",
    "        dst_fn = os.path.join('data/pseudo/audio/unknown', bn)\n",
    "        copy(src_fn, dst_fn)\n",
    "# when using as pseudo labels remove 'silence'! (e.g. ls *.wav | grep -v silence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show (predicted) test data distribution\n",
    "### Note that during training silence prob was 10%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_bar(submission):\n",
    "    counts = []\n",
    "    for label_name in submission.label.unique():\n",
    "        label_count = (submission.label == label_name).sum()\n",
    "        percent = (label_count / num_total) * 100.0\n",
    "        counts.append((label_name, percent))\n",
    "    print(counts)\n",
    "    plt.bar(range(len(counts)), [c[-1] for c in counts])\n",
    "    _ = plt.xticks(range(len(counts)), [c[0] for c in counts])\n",
    "    plt.grid('on')\n",
    "    plt.xlabel('Label names')\n",
    "    plt.ylabel('Precentage [%]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_bar(sub1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compress and decompress using mu-law"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_wav(fn):\n",
    "    rate, data = wf.read(fn)\n",
    "    assert rate == 16000\n",
    "    data = data / 32768\n",
    "    missing = 16000 - len(data)\n",
    "    data = np.pad(data, (missing, 0), mode='constant')\n",
    "    return data\n",
    "\n",
    "\n",
    "def compress(x, mu=255):\n",
    "    assert x.min() >= -1 and x.max() <= 1\n",
    "    compressed = np.sign(x) * np.log10(1.0 + mu * np.abs(x)) / np.log10(1.0 + mu)\n",
    "    return compressed\n",
    "\n",
    "def decompress(x, mu=255):\n",
    "    assert x.min() >= -1 and x.max() <= 1\n",
    "    decompressed= np.sign(x) * (1.0 / mu) * (np.power(1.0 + x, np.abs(x)) - 1.0)\n",
    "    return decompressed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "up_sample = load_wav('data/test/audio/clip_001204892.wav')\n",
    "float_audio(up_sample)\n",
    "c = compress(up_sample)\n",
    "d = decompress(c)\n",
    "float_audio(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Audio(data=np.int16(up_sample  * 32768), rate=16000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use average pooling to reduce signal size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import AveragePooling1D, Input, Reshape\n",
    "def avg_model(sample_rate=16000):\n",
    "    input_layer = Input([sample_rate])\n",
    "    x = input_layer\n",
    "    x = Reshape([-1, 1])(x)\n",
    "    x = AveragePooling1D(2)(x)\n",
    "    return Model(input_layer, x)\n",
    "\n",
    "model = avg_model()\n",
    "out = model.predict([up_sample.reshape((1, -1))]).squeeze()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.title('Orginal')\n",
    "plt.plot(up_sample)\n",
    "plt.figure()\n",
    "plt.title('Subsamples')\n",
    "plt.plot(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "float_audio(out, sample_rate=len(out))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How is the padding handeled by decode_wav?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.contrib.framework.python.ops import audio_ops as contrib_audio\n",
    "from tensorflow.python.ops import io_ops\n",
    "\n",
    "wav_filename_placeholder = tf.placeholder(tf.string, [])\n",
    "wav_loader = io_ops.read_file(wav_filename_placeholder)\n",
    "wav_decoder = contrib_audio.decode_wav(\n",
    "    wav_loader, desired_channels=1)\n",
    "fns = sorted(glob('data/train/audio/go/*.wav'))\n",
    "short_fn = ''\n",
    "with tf.Session() as sess:\n",
    "    for fn in fns:\n",
    "        out = sess.run(wav_decoder.audio,\n",
    "                       {wav_filename_placeholder: fn_val})\n",
    "        if out.shape[0] != 16000:\n",
    "            short_fn = fn\n",
    "            print(fn)\n",
    "            break\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Audio(short_fn, autoplay=True))\n",
    "sleep(1)\n",
    "float_audio(out, autoplay=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check how many samples are shorter than 1s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "train_fns = sorted(glob('data/train/audio/*/*.wav'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_wav(fn):\n",
    "    rate, data = wf.read(fn)\n",
    "    assert rate == 16000\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "short_count = 0\n",
    "for fn in tqdm(train_fns):\n",
    "    data = load_wav(fn)\n",
    "    if len(data) != 16000:\n",
    "        short_count += 1\n",
    "print(\"Short: \", short_count)\n",
    "print(\"All: \", len(train_fns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize augmented training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division, print_function\n",
    "from keras import backend as K\n",
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
    "from keras.callbacks import TensorBoard\n",
    "from callbacks import ConfusionMatrixCallback\n",
    "from model import speech_model, prepare_model_settings\n",
    "from input_data import AudioProcessor, prepare_words_list\n",
    "from classes import get_classes\n",
    "from IPython import embed  # noqa"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def data_gen(audio_processor, sess,\n",
    "             batch_size=128,\n",
    "             background_frequency=0.5, background_volume_range=0.2,\n",
    "             foreground_frequency=0.5, foreground_volume_range=0.2,\n",
    "             time_shift=(100.0 * 16000.0) / 1000,\n",
    "             mode='validation'):\n",
    "    offset = 0\n",
    "    if mode != 'training':\n",
    "        background_frequency = 0.0\n",
    "        background_volume_range = 0.0\n",
    "        foreground_frequency = 0.0\n",
    "        foreground_volume_range = 0.0\n",
    "        time_shift = 0\n",
    "    while True:\n",
    "        X, y = audio_processor.get_data(\n",
    "            how_many=batch_size, offset=0 if mode == 'training' else offset,\n",
    "            background_frequency=background_frequency,\n",
    "            background_volume_range=background_volume_range,\n",
    "            foreground_frequency=foreground_frequency,\n",
    "            foreground_volume_range=foreground_volume_range,\n",
    "            time_shift=time_shift, mode=mode, sess=sess)\n",
    "        offset += batch_size\n",
    "        if offset > ap.set_size(mode) - batch_size:\n",
    "              offset = 0\n",
    "        yield X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = K.get_session()\n",
    "data_dirs = ['data/train/audio']\n",
    "add_pseudo = True\n",
    "if add_pseudo:\n",
    "    data_dirs.append('data/pseudo/audio')\n",
    "compute_mfcc = False\n",
    "sample_rate = 16000\n",
    "batch_size = 100\n",
    "classes = get_classes(wanted_only=False)\n",
    "model_settings = prepare_model_settings(\n",
    "  label_count=len(prepare_words_list(classes)), sample_rate=sample_rate,\n",
    "  clip_duration_ms=1000, window_size_ms=30.0, window_stride_ms=10.0,\n",
    "  dct_coefficient_count=40)\n",
    "ap = AudioProcessor(\n",
    "  data_dirs=data_dirs,\n",
    "  silence_percentage=10.0,\n",
    "  unknown_percentage=7.0,\n",
    "  wanted_words=classes,\n",
    "  validation_percentage=10.0,\n",
    "  testing_percentage=0.0,\n",
    "  model_settings=model_settings,\n",
    "  compute_mfcc=compute_mfcc)\n",
    "train_gen = data_gen(ap, sess, batch_size=batch_size, mode='training')\n",
    "val_gen = data_gen(ap, sess, batch_size=batch_size, mode='validation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ap.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disp_count = 0\n",
    "while True:\n",
    "    if disp_count > 5:\n",
    "        break\n",
    "    X, y = next(train_gen)\n",
    "    for i in range(X.shape[0]):\n",
    "        if y[i].argmax() == 10:\n",
    "            sample = X[i, :].squeeze()\n",
    "            float_audio(sample, autoplay=True)\n",
    "            sleep(1)\n",
    "            disp_count += 1\n",
    "            # plt.figure()\n",
    "            # plt.axis([0, 16000, -1, 1])\n",
    "            # plt.plot(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create pseudo labels from consistent predictions\n",
    "### [v_001]: Sub1: 84% PLB, Sub2: 84% PLB, Sub3: 82%: 118078 consistend pseudo labels\n",
    "### [v_002]: Sub1: 87%, Sub2: 87%, Sub3: 88%: 140945 pseudo consistend pseudo labels\n",
    "#### v_002: There are only 158538 test data points! Maybe that's the reason I am stuck -> reduce pseudo label fraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub1 = pd.read_csv('submission_098_leftloud_tta_all_labels.csv')  # 87% PLB\n",
    "sub2 = pd.read_csv('submission_096_leftloud_tta_all_labels.csv')  # 87% PLB\n",
    "sub3 = pd.read_csv('submission_091_leftloud_tta_all_labels.csv')  # 88% PLB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "consistend = ((sub1.label == sub2.label) & (sub1.label == sub3.label))\n",
    "print(\"All: \", sub1.shape[0], \" consistend: \", consistend.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in tqdm(range(sub1.shape[0])):\n",
    "    fn = sub1.loc[i, 'fname']\n",
    "    if fn != sub2.loc[i, 'fname'] or fn != sub3.loc[i, 'fname']:\n",
    "        print(\"Fatal error\")\n",
    "        break\n",
    "    if consistend[i]:\n",
    "        label = sub1.loc[i, 'label']\n",
    "        dst_fn = jp('data', 'pseudo', 'audio', label, fn)\n",
    "        src_fn = jp('data', 'test', 'audio', fn)\n",
    "        copy(src_fn, dst_fn, follow_symlinks=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create new background noise from pseudo silence\n",
    "## Just concat 60 seconds of audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pseudo_silence_fns = sorted(glob('data/pseudo/silence/*.wav'))\n",
    "num_pseudo_noise = 5\n",
    "num_secs = 60\n",
    "for i in range(num_pseudo_noise):\n",
    "    background_fns = np.random.choice(\n",
    "        pseudo_silence_fns, num_secs, replace=False)\n",
    "    new_clip = []\n",
    "    for fn in background_fns:\n",
    "        rate, data = wf.read(fn)\n",
    "        new_clip.append(data)\n",
    "    new_clip = np.concatenate(new_clip).astype(np.int16)\n",
    "    out_fn = 'data/train/audio/_background_noise_/custom_pseudo_silence_%04d.wav' % i\n",
    "    # print(out_fn)\n",
    "    wf.write(out_fn, 16000, new_clip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "?wf.write"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Listen to submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = get_classes(wanted_only=False, extend_reversed=True)\n",
    "[w for w in words if 'b' in w]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sub = pd.read_csv('sub')\n",
    "offset = 1000\n",
    "label2int = get_label2int(wanted_only=True)\n",
    "disp_count = 0\n",
    "for i in list(reversed(range(sub.shape[0])))[offset:]:\n",
    "    if disp_count >= 10:\n",
    "        break\n",
    "    fn = jp('data', 'test', 'audio', sub.loc[i, 'fname'])\n",
    "    label = sub.loc[i, 'label']\n",
    "    if label == 'off' and np.random.rand() > 0.8:\n",
    "        print(fn, label)\n",
    "        display(Audio(fn, autoplay=True))\n",
    "        sleep(1)\n",
    "        disp_count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "np.random.seed(5)\n",
    "sub = pd.read_csv('submission_034_all_labels.csv')\n",
    "label2int = get_label2int(wanted_only=True)\n",
    "disp_count = 0\n",
    "for i in reversed(range(sub.shape[0])):\n",
    "    if disp_count >= 2:\n",
    "        break\n",
    "    label = sub.loc[i, 'label']\n",
    "    if label == 'silence' and np.random.rand() > 0.999:\n",
    "        fn = jp('data', 'test', 'audio', sub.loc[i, 'fname'])\n",
    "        rate, data = wf.read(fn)\n",
    "        data = np.float32(data) / 32767\n",
    "        # data = np.sqrt(data) + 0.5\n",
    "        data = 10 ** (data) - 1.0\n",
    "        plot_audio(data, normed=True)\n",
    "        print(fn, label)\n",
    "        float_audio(data, autoplay=True)\n",
    "        sleep(1)\n",
    "        disp_count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare two submissions\n",
    "# sub1 = pd.read_csv('submission_091_all_labels.csv')\n",
    "# sub2 = pd.read_csv('submission_091_4x_tta_all_labels.csv')\n",
    "\n",
    "sub1 = pd.read_csv('submission_091_all_labels.csv')\n",
    "sub2 = pd.read_csv('submission_091_leftloud_tta_all_labels.csv')\n",
    "num_different = 0\n",
    "\n",
    "for i in tqdm(reversed(range(sub1.shape[0]))):\n",
    "    fn = sub1.loc[i, 'fname']\n",
    "    assert fn == sub2.loc[i, 'fname']\n",
    "    label1 = sub1.loc[i, 'label']\n",
    "    label2 = sub2.loc[i, 'label']\n",
    "    if label1 != label2:\n",
    "        num_different += 1\n",
    "        print(\"%s vs. %s\" % (label1, label2))\n",
    "        in_fn = jp('data/test/audio', fn)\n",
    "        display(Audio(in_fn, autoplay=True))\n",
    "        sleep(1)\n",
    "print(\"%d of %d are different\" % (num_different, sub1.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Listen to submission with probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Wanted: \", get_classes(wanted_only=True))\n",
    "print(\"All: \", get_classes(wanted_only=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sub = pd.read_csv('submission_100_leftloud_tta_all_labels_probs.csv')\n",
    "label2int = get_label2int(wanted_only=False)\n",
    "int2label = get_int2label(wanted_only=False)\n",
    "disp_count = 0\n",
    "for i in reversed(range(sub.shape[0])):\n",
    "    if disp_count >= 5:\n",
    "        break\n",
    "    fn = jp('data', 'test', 'audio', sub.loc[i, 'fname'])\n",
    "    label = sub.loc[i, 'label']\n",
    "    if label == 'no' and np.random.rand() > 0.99:\n",
    "        probs = sub.loc[i, label2int.keys()]\n",
    "        max_prob = probs.max()\n",
    "        if max_prob < 0.5:\n",
    "            plt.figure()\n",
    "            plt.title(str(disp_count))\n",
    "            plt.bar(range(len(probs)), probs)\n",
    "            plt.axis([0, len(probs), 0, 1])\n",
    "            plt.xticks(range(len(probs)), label2int.keys(), rotation='vertical')\n",
    "            display(Audio(fn, autoplay=True))\n",
    "            sleep(1)\n",
    "            disp_count += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correct broken submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_broken = pd.read_csv('submission_012.csv')\n",
    "sub_all = pd.read_csv('submission_012_all_labels.csv')\n",
    "sub_broken.loc[(sub_all.label == 'unknown').values, 'label'] = 'silence'\n",
    "sub_broken.to_csv('submission_012_corrected.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = pd.read_csv('submission_012_corrected.csv')\n",
    "disp_count = 0\n",
    "for i in range(sub.shape[0]):\n",
    "    if disp_count >= 10:\n",
    "        break\n",
    "    fn = jp('data', 'test', 'audio', sub.loc[i, 'fname'])\n",
    "    label = sub.loc[i, 'label']\n",
    "    if label != 'unknown' and np.random.rand() > 0.9:\n",
    "        print(label)\n",
    "        display(Audio(fn, autoplay=True))\n",
    "        sleep(1)\n",
    "        disp_count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = pd.read_csv('submission_012_all_labels.csv')\n",
    "counts = sub.label.value_counts()\n",
    "percent_counts = counts.apply(lambda x: np.round(100.0 * (x / sub.shape[0])))\n",
    "# print(counts)\n",
    "print(percent_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check what the generators are producing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division, print_function\n",
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
    "from keras.callbacks import TensorBoard\n",
    "from callbacks import ConfusionMatrixCallback\n",
    "from model import speech_model, prepare_model_settings\n",
    "from input_data import AudioProcessor, prepare_words_list\n",
    "from classes import get_classes\n",
    "from IPython import embed  # noqa\n",
    "\n",
    "\n",
    "def data_gen(audio_processor, sess,\n",
    "             batch_size=128,\n",
    "             background_frequency=0.5, background_volume_range=0.2,\n",
    "             foreground_frequency=0.5, foreground_volume_range=0.2,\n",
    "             time_shift_frequency=1.0, time_shift_range=[-2000, 0],\n",
    "             mode='validation', pseudo_frequency=0.4):\n",
    "  offset = 0\n",
    "  if mode != 'training':\n",
    "    background_frequency = 0.0\n",
    "    background_volume_range = 0.0\n",
    "    foreground_frequency = 0.0\n",
    "    foreground_volume_range = 0.0\n",
    "    pseudo_frequency = 0.0\n",
    "    time_shift_frequency = 0.0\n",
    "    time_shift_range = [0, 0]\n",
    "  while True:\n",
    "    X, y = audio_processor.get_data(\n",
    "        how_many=batch_size, offset=0 if mode == 'training' else offset,\n",
    "        background_frequency=background_frequency,\n",
    "        background_volume_range=background_volume_range,\n",
    "        foreground_frequency=foreground_frequency,\n",
    "        foreground_volume_range=foreground_volume_range,\n",
    "        time_shift_frequency=time_shift_frequency,\n",
    "        time_shift_range=time_shift_range,\n",
    "        mode=mode, sess=sess,\n",
    "        pseudo_frequency=pseudo_frequency)\n",
    "    offset += batch_size\n",
    "    if offset > ap.set_size(mode) - batch_size:\n",
    "      offset = 0\n",
    "    yield X, y\n",
    "\n",
    "\n",
    "# running_mean: -0.8 | running_std: 7.0\n",
    "# mfcc running_mean: -0.67 | running_std: 7.45\n",
    "# background_clamp running_mean: -0.00064 | running_std: 0.0774, p5: -0.074, p95: 0.0697  # noqa\n",
    "# 10 ** raw - 1.0 running_mean: 0.017 | 10 ** raw - 1.0 running_std: 0.28\n",
    "# np.log(11) ~ 2.4\n",
    "# np.log(12) ~ 2.5\n",
    "# np.log(32) ~ 3.5\n",
    "# np.log(48) ~ 3.9\n",
    "# 64727 training files\n",
    "if __name__ == '__main__':\n",
    "  # restrict gpu usage: https://stackoverflow.com/questions/34199233/how-to-prevent-tensorflow-from-allocating-the-totality-of-a-gpu-memory  # noqa\n",
    "  gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.90)\n",
    "  sess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options))\n",
    "  K.set_session(sess)\n",
    "  data_dirs = ['data/train/audio']\n",
    "  add_pseudo = True\n",
    "  if add_pseudo:\n",
    "    data_dirs.append('data/pseudo/audio')\n",
    "  output_representation = 'raw'\n",
    "  sample_rate = 16000\n",
    "  batch_size = 384\n",
    "  classes = get_classes(wanted_only=False, extend_reversed=False)\n",
    "  model_settings = prepare_model_settings(\n",
    "      label_count=len(prepare_words_list(classes)), sample_rate=sample_rate,\n",
    "      clip_duration_ms=1000, window_size_ms=30.0, window_stride_ms=10.0,\n",
    "      dct_coefficient_count=40)\n",
    "  ap = AudioProcessor(\n",
    "      data_dirs=data_dirs, wanted_words=classes,\n",
    "      silence_percentage=15.0, unknown_percentage=5.0,\n",
    "      validation_percentage=10.0, testing_percentage=0.0,\n",
    "      model_settings=model_settings,\n",
    "      output_representation=output_representation)\n",
    "  train_gen = data_gen(ap, sess, batch_size=batch_size, mode='training')\n",
    "  val_gen = data_gen(ap, sess, batch_size=batch_size, mode='validation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(int2label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "int2label = get_int2label(wanted_only=False)\n",
    "X, y = next(train_gen)\n",
    "for j in range(X.shape[0]):\n",
    "    label = int2label[y[j].argmax()]\n",
    "    if label == 'sheila':\n",
    "        data = X[j, :].squeeze()\n",
    "        float_audio(data, autoplay=True)\n",
    "        sleep(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ap.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wanted_words = get_classes(wanted_only=True)\n",
    "print(wanted_words)\n",
    "labels2int = {v: k for k, v in ap.word_to_index.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf data/train/audio/_silence_\n",
    "!mkdir data/train/audio/_silence_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# save silence samples to '_silence_'\n",
    "disp_count = 0\n",
    "for i in range(100):\n",
    "    print(disp_count)\n",
    "    if disp_count > 100:\n",
    "        break\n",
    "    X, y = next(train_gen)\n",
    "    y = y.argmax(axis=-1)\n",
    "    for j in range(len(y)):\n",
    "        if y[j] == 0:\n",
    "            print(labels2int[y[j]])\n",
    "            out_fn = jp('data/train/audio/_silence_/%05d.wav' % disp_count)\n",
    "            data = np.int16(X[j, :] * 32768)\n",
    "            wf.write(out_fn, 16000, data)\n",
    "            # float_audio(X[j, :], autoplay=True)\n",
    "            # sleep(1)\n",
    "            disp_count += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load model and check where we go wrong in the validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = speech_model(\n",
    "  'conv_1d_time',\n",
    "  model_settings['fingerprint_size'] if compute_mfcc else sample_rate,\n",
    "  num_classes=model_settings['label_count'])\n",
    "model.load_weights('checkpoints_014/ep-039-loss-0.173.hdf5')\n",
    "disp_count = 0\n",
    "for i in range(ap.set_size('validation') // batch_size):\n",
    "    if disp_count >= 10:\n",
    "        break\n",
    "    X, y_true = next(val_gen)\n",
    "    y_true = y_true.argmax(axis=-1)\n",
    "    y_pred = model.predict(X).argmax(axis=-1)\n",
    "    for j in range(len(y_true)):\n",
    "        if y_true[j] != y_pred[j] and np.random.rand() > 0.5:\n",
    "            l_true = labels2int[y_true[j]]\n",
    "            l_pred = labels2int[y_pred[j]]\n",
    "            print(\"Pred: %s, Actual: %s\" % (l_pred, l_true))\n",
    "            float_audio(X[j, :], autoplay=True)\n",
    "            sleep(1)\n",
    "            disp_count += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create \"unknown\" words\n",
    "### Just reverse some of the unwanted words. Reversing the wanted words might actually cause some problems when using global pooling in the end?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(exclude_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wanted_words = get_classes(wanted_only=True)\n",
    "# wow reversed is wow! one reversed sounds like no!\n",
    "# dog vs go :P\n",
    "exclude_words = wanted_words + ['_background_noise_', 'wow', 'one', 'dog']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fns = sorted(glob(jp('data', 'train', 'audio', '*', '*.wav')))\n",
    "print(len(fns))\n",
    "fns = [fn for fn in fns if fn.split('/')[-2] not in exclude_words]\n",
    "print(len(fns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_classes = list(set([fn.split('/')[-2] for fn in fns]))\n",
    "# new_classes.remove('_silence_')\n",
    "new_classes = ['new_' + new_class[::-1] for new_class in new_classes]\n",
    "print(new_classes)\n",
    "for new_class in new_classes:\n",
    "    new_dir = jp('data/train/audio', new_class)\n",
    "    if os.path.exists(new_dir):\n",
    "        rmtree(new_dir)\n",
    "    os.mkdir(new_dir)\n",
    "    print(\"created %s\" % new_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.shuffle(fns)\n",
    "# fn_selection = np.random.choice(fns, size=5000, replace=False)\n",
    "fn_selection = fns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# there (often) 5 times the same speaker said the same word\n",
    "# \n",
    "# ./eight/5fadb538_nohash_4.wav\n",
    "# ./eight/5fadb538_nohash_3.wav\n",
    "# ./eight/5fadb538_nohash_2.wav\n",
    "# ./eight/5fadb538_nohash_1.wav\n",
    "# ./eight/5fadb538_nohash_0.wav\n",
    "\n",
    "counter = {cn: 0 for cn in new_classes}\n",
    "for fn in tqdm(fn_selection):\n",
    "    cn = 'new_' + fn.split('/')[-2][::-1]\n",
    "    sample_rate, data = wf.read(fn)\n",
    "    bn = os.path.basename(fn)\n",
    "    dst = jp('data', 'train', 'audio', cn,\n",
    "             \"%06d.wav\" % counter[cn])\n",
    "    counter[cn] += 1\n",
    "    data = data[::-1]\n",
    "    wf.write(dst, sample_rate, data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Take a look at the noise files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "background_files = sorted(glob(jp('data', 'train', 'audio', '_background_noise_', '*.wav')))\n",
    "background_volumne = 0.8\n",
    "for fn in background_files[5:6]:\n",
    "    print(fn)\n",
    "    rate, data = wf.read(fn)\n",
    "    if len(data) > 16000:\n",
    "        data = data[:16000]\n",
    "    data = np.float32(data) / 32768\n",
    "    data *= background_volumne\n",
    "    print(data.max())\n",
    "    wf.write('tmp.wav', 16000, np.int16(data * 32767))\n",
    "    plot_audio(data, normed=True)\n",
    "    display(Audio('tmp.wav', autoplay=True))\n",
    "    sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "?wf.write"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Take a look at \"happy\" files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "background_files = sorted(glob(jp('data', 'train', 'audio', 'happy', '*.wav')))\n",
    "volumne = 10**1\n",
    "for fn in background_files[5:6]:\n",
    "    print(fn)\n",
    "    rate, data = wf.read(fn)\n",
    "    if len(data) > 16000:\n",
    "        data = data[:16000]\n",
    "    data = np.float32(data) / 32768\n",
    "    data *= volumne\n",
    "    data = np.clip(data, -1, 1)\n",
    "    print(data.max())\n",
    "    wf.write('tmp.wav', 16000, np.int16(data * 32767))\n",
    "    plot_audio(data, normed=True)\n",
    "    display(Audio('tmp.wav', autoplay=True))\n",
    "    sleep(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Majority vote submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from os.path import join as jp\n",
    "from shutil import copy\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "sub_fns = ['submission_011.csv', 'submission_017.csv',\n",
    "           'submission_018.csv', 'submission_014.csv',\n",
    "           'submission_020.csv']\n",
    "subs = [pd.read_csv(sub_fn) for sub_fn in sub_fns]\n",
    "\n",
    "fname, label = [], []\n",
    "clear_majority = 0\n",
    "for i in tqdm(range(subs[0].shape[0])):\n",
    "    fname.append(subs[0].loc[i, 'fname'])\n",
    "    label_counts = {}\n",
    "    for sub in subs:\n",
    "        ll = sub.loc[i, 'label']\n",
    "    if ll in label_counts:\n",
    "        label_counts[ll] += 1\n",
    "    else:\n",
    "        label_counts[ll] = 1\n",
    "\n",
    "    maj_label = max(label_counts, key=label_counts.get)\n",
    "    if label_counts[maj_label] > 2:\n",
    "        clear_majority += 1\n",
    "    else:\n",
    "        # in trouble save the wav files!\n",
    "        src = jp('data', 'test', 'audio', fname[-1])\n",
    "        dst = jp('split_decision', str(label_counts) + fname[-1])\n",
    "        copy(src, dst)\n",
    "    # resolve tie by chosing 'unknown' or 'silence' if available\n",
    "    if 'unknown' in label_counts and 'silence' in label_counts:\n",
    "        maj_label = 'silence'\n",
    "    elif 'unknown' in label_counts:\n",
    "        maj_label = 'unknown'\n",
    "    elif 'silence' in label_counts:\n",
    "        maj_label = 'silence'\n",
    "    label.append(maj_label)\n",
    "\n",
    "pd.DataFrame({'fname': fname, 'label': label}).to_csv(\n",
    "    'majority_sub_011.csv', index=False)\n",
    "print(\"Done! Got a clear majority for %d of %d samples.\"\n",
    "      % (clear_majority, subs[0].shape[0]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate more background noise\n",
    "## The idea is to take the reversed words and average some of them up to generate ~30s tracks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unwanted words reversed\n",
    "fns = sorted(glob(jp('data', 'train', 'audio', 'unwrev', '*.wav')))\n",
    "print(len(fns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "louder = 16.0  # make it louder!\n",
    "sample_secs = 30\n",
    "total_new = 0\n",
    "num_noise_tracks = 400\n",
    "noise_track = []\n",
    "noise_track_counter = 0\n",
    "sample_track = []\n",
    "sample_counter = 0\n",
    "for fn in tqdm(fns):\n",
    "    sample_rate, data = wf.read(fn)\n",
    "    data = np.float32(data) / 32768\n",
    "    assert sample_rate == 16000\n",
    "    data = center_pad(data)\n",
    "    shift = np.random.randint(16000)\n",
    "    data = np.roll(data, shift)\n",
    "    sample_track.append(data)\n",
    "    sample_counter += 1\n",
    "    if sample_counter == sample_secs:\n",
    "        noise_track.append(np.concatenate(sample_track))\n",
    "        sample_track = []\n",
    "        sample_counter = 0\n",
    "        noise_track_counter += 1\n",
    "        if noise_track_counter == num_noise_tracks:\n",
    "            noise_track = np.array(noise_track)\n",
    "            noise_track = np.mean(noise_track, axis=0) * louder\n",
    "            noise_track = np.int16(noise_track * 32768)\n",
    "            out_fn = jp('data', 'train', 'audio', '_background_noise_', 'silence_please_%04d.wav' % total_new)\n",
    "            wf.write(out_fn, 16000, noise_track)\n",
    "            print(out_fn)\n",
    "            display(Audio(out_fn))\n",
    "            noise_track = []\n",
    "            noise_track_counter = 0\n",
    "            total_new += 1\n",
    "            if total_new == 2:\n",
    "                break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How does foreground with mixed background sound?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "background_noise_fns = sorted(glob('data/train/audio/_background_noise_/*.wav'))\n",
    "idx = -3\n",
    "print(background_noise_fns[idx])\n",
    "display(Audio(background_noise_fns[idx], autoplay=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "foreground_fns = sorted(glob('data/train/audio/go/*.wav'))\n",
    "for i in range(3):\n",
    "    foreground_fn = np.random.choice(foreground_fns)\n",
    "    foreground = normalized_read(foreground_fn)\n",
    "    background_fn = np.random.choice(background_noise_fns)\n",
    "    # background_fn = background_noise_fns[-3]\n",
    "    print(background_fn)\n",
    "    background = normalized_read(background_fn)\n",
    "    mixed = np.random.uniform(0.8, 1.2) * foreground + 0.3 * background  # np.random.uniform(0, 2)\n",
    "    mixed = np.clip(mixed, -1, 1)\n",
    "    float_audio(mixed, autoplay=True)\n",
    "    plot_audio(mixed, normed=True)\n",
    "    sleep(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How do the models perform on the validation set?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
    "from keras.callbacks import TensorBoard\n",
    "from model import speech_model, prepare_model_settings\n",
    "from input_data import AudioProcessor, prepare_words_list\n",
    "from keras.models import load_model\n",
    "from classes import get_classes\n",
    "\n",
    "\n",
    "def data_gen(audio_processor, sess,\n",
    "             batch_size=128,\n",
    "             background_frequency=0.5, background_volume_range=0.2,\n",
    "             foreground_frequency=1.0, foreground_volume_range=3.0,\n",
    "             time_shift=(100.0 * 16000.0) / 1000,\n",
    "             mode='validation'):\n",
    "    offset = 0\n",
    "    if mode != 'training':\n",
    "        background_frequency = 0.0\n",
    "        background_volume_range = 0.0\n",
    "        foreground_frequency = 0.0\n",
    "        foreground_volume_range = 0.0\n",
    "        time_shift = 0\n",
    "\n",
    "    while True:\n",
    "        X, y = audio_processor.get_data(\n",
    "            how_many=batch_size, offset=0 if mode == 'training' else offset,\n",
    "            background_frequency=background_frequency,\n",
    "            background_volume_range=background_volume_range,\n",
    "            foreground_frequency=foreground_frequency,\n",
    "            foreground_volume_range=foreground_volume_range,\n",
    "            time_shift=time_shift, mode=mode, sess=sess)\n",
    "        offset += batch_size\n",
    "        if offset > ap.set_size(mode) - batch_size:\n",
    "            offset = 0\n",
    "        yield X, y\n",
    "\n",
    "\n",
    "sess = K.get_session()\n",
    "data_dirs = ['data/train/audio']\n",
    "add_pseudo = False\n",
    "if add_pseudo:\n",
    "    data_dirs.append('data/pseudo/audio')\n",
    "compute_mfcc = False\n",
    "sample_rate = 16000\n",
    "batch_size = 64\n",
    "classes = get_classes(wanted_only=False)\n",
    "model_settings = prepare_model_settings(\n",
    "  label_count=len(prepare_words_list(classes)), sample_rate=sample_rate,\n",
    "  clip_duration_ms=1000, window_size_ms=30.0, window_stride_ms=10.0,\n",
    "  dct_coefficient_count=40)\n",
    "ap = AudioProcessor(\n",
    "  data_dirs=data_dirs,\n",
    "  silence_percentage=15.0,\n",
    "  unknown_percentage=7.0,\n",
    "  wanted_words=classes,\n",
    "  validation_percentage=10.0,\n",
    "  testing_percentage=0.0,\n",
    "  model_settings=model_settings,\n",
    "  compute_mfcc=compute_mfcc)\n",
    "train_gen = data_gen(ap, sess, batch_size=batch_size, mode='training')\n",
    "val_gen = data_gen(ap, sess, batch_size=batch_size, mode='validation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_fns = [\n",
    "    'checkpoints_019/ep-022-vl-0.2916.hdf5',\n",
    "    'checkpoints_018/ep-049-vl-0.2185.hdf5',\n",
    "    'checkpoints_017/ep-036-vl-0.1969.hdf5'\n",
    "]\n",
    "\n",
    "models = []\n",
    "for model_fn in model_fns:\n",
    "    model = load_model(model_fn)\n",
    "    models.append((model_fn, model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate on validation set\n",
    "for model in models:\n",
    "    out = model.evaluate_generator(\n",
    "        val_gen, steps=ap.set_size('validation') // batch_size)\n",
    "    print(model_fn, out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict samples with models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(ap.words_list), len(models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = next(val_gen)\n",
    "sample_idx = 10\n",
    "float_audio(X[sample_idx, :], autoplay=True)\n",
    "out_probs = []\n",
    "for model_fn, model in models:\n",
    "    y_preds = model.predict(X)\n",
    "    out_probs.append((model_fn, y_preds[sample_idx]))\n",
    "\n",
    "for fn, pred in out_probs:\n",
    "    plt.figure()\n",
    "    plt.title(fn)\n",
    "    plt.bar(range(len(pred)), pred)\n",
    "    plt.xticks(range(len(pred)), ap.words_list, rotation='vertical')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wrong ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_plots = 2\n",
    "plot_c = 0\n",
    "model = models[-1][-1]\n",
    "while plot_c < max_plots:\n",
    "    X, y = next(val_gen)\n",
    "    y_preds = model.predict(X)\n",
    "    for i, y_pred in enumerate(y_preds):\n",
    "        if plot_c >= max_plots:\n",
    "            break\n",
    "        if y_pred.argmax() != y[i].argmax():\n",
    "            float_audio(X[i, :], autoplay=True)\n",
    "            sleep(1)\n",
    "            plt.figure()\n",
    "            plt.title(\"Pred: %s, Actual: %s\"\n",
    "                      % (ap.words_list[y_pred.argmax()], ap.words_list[y[i].argmax()]))\n",
    "            plt.bar(range(len(y_pred)), y_pred)\n",
    "            plt.xticks(range(len(y_pred)), ap.words_list, rotation='vertical')\n",
    "            plot_c += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What happens if we just feed plain silence e.g. all 0?\n",
    "### Looks good!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.zeros((2, 16000), dtype=np.float32)\n",
    "pred = model.predict(X)\n",
    "float_audio(X[0, :], autoplay=True)\n",
    "sleep(1)\n",
    "plt.figure()\n",
    "plt.title(\"Silence pred\")\n",
    "plt.bar(range(len(pred[0])), pred[0])\n",
    "_ = plt.xticks(range(len(pred[0])), ap.words_list, rotation='vertical')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How about the noise files?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model(model_fns[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_fns = glob(jp('data/train/audio/_background_noise_/*.wav'))\n",
    "X = []\n",
    "for noise_fn in noise_fns:\n",
    "    X.append(normalized_read(noise_fn))\n",
    "X = np.float32(X)\n",
    "# change volumne\n",
    "X *= 0.2\n",
    "y_pred = model.predict(X)\n",
    "for i in range(X.shape[0]):\n",
    "    # float_audio(X[0, :], autoplay=True)\n",
    "    # sleep(1)\n",
    "    plt.figure()\n",
    "    plt.title(\"Noise pred: %s\" % noise_fns[i])\n",
    "    plt.bar(range(len(y_pred[i])), y_pred[i])\n",
    "    _ = plt.xticks(range(len(y_pred[i])), ap.words_list, rotation='vertical')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission with averaged probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_fns = [\n",
    "    'submission_091_leftloud_tta.csv',\n",
    "    'submission_106_tta_leftloud.csv']\n",
    "sub_fns = [fn.replace('.csv', '_all_labels_probs.csv')\n",
    "           for fn in sub_fns]\n",
    "\n",
    "subs = []\n",
    "for sub_fn in sub_fns:\n",
    "    subs.append(pd.read_csv(sub_fn))\n",
    "\n",
    "wanted_words = prepare_words_list(get_classes(wanted_only=True))\n",
    "int2label = get_int2label(wanted_only=False)\n",
    "label2int = get_label2int(wanted_only=False)\n",
    "\n",
    "avg = 0.0\n",
    "for sub in subs:\n",
    "    avg += sub.loc[:, label2int.keys()].values / len(subs)\n",
    "\n",
    "print(avg.sum(axis=1)[10:20])\n",
    "probabilities = avg\n",
    "print(probabilities.shape)\n",
    "preds = probabilities.argmax(axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_label = []\n",
    "ensemble_sub = subs[0].copy()\n",
    "for i in tqdm(range(ensemble_sub.shape[0])):\n",
    "    label = int2label[preds[i]]\n",
    "    # print(\"%s with: %.3f\" % (label, probabilities[i, preds[i]]))\n",
    "    label = label if label != '_silence_' else 'silence'\n",
    "    label = label if label != '_unknown_' else 'unknown'\n",
    "    # label = label if label == 'silence' or label in wanted_words else 'unknown'\n",
    "    ensemble_label.append(label)\n",
    "\n",
    "ensemble_sub['label'] = ensemble_label\n",
    "ensemble_sub[['fname', 'label']].to_csv('prob_ensemble_026_all_labels.csv', index=False)\n",
    "ensemble_sub.loc[:, label2int.keys()] = probabilities\n",
    "ensemble_sub.to_csv('prob_ensemble_026_all_labels_probs.csv', index=False)\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_label = []\n",
    "ensemble_sub = subs[0].copy()\n",
    "small_prob_counter = 0\n",
    "prob_thresh = 0.2\n",
    "for i in tqdm(range(ensemble_sub.shape[0])):\n",
    "    label = int2label[preds[i]]\n",
    "    label = label if label != '_silence_' else 'silence'\n",
    "    label = label if label != '_unknown_' else 'unknown'\n",
    "    label = label if label == 'silence' or label in wanted_words else 'unknown'\n",
    "    # handle \"small\" probs\n",
    "    max_prob = probabilities[i, preds[i]]\n",
    "    if max_prob < prob_thresh:\n",
    "        small_prob_counter += 1\n",
    "        src_fn = jp('data/test/audio', ensemble_sub.loc[i, 'fname'])\n",
    "        dst_fn = jp('split_decision', '%s_%.3f.wav'\n",
    "                    % (label, max_prob))\n",
    "        copy(src_fn, dst_fn)\n",
    "        # map small prob words (not silence) to unknown\n",
    "        label = label if label == 'silence' else 'unknown'\n",
    "    ensemble_label.append(label)\n",
    "\n",
    "\n",
    "print(\"%d of %d with small prob\" % (small_prob_counter, ensemble_sub.shape[0]))\n",
    "ensemble_sub['label'] = ensemble_label\n",
    "ensemble_sub[['fname', 'label']].to_csv(\n",
    "    'prob_ensemble_026_thresh_%.2f_map_to_unknown.csv' % prob_thresh, index=False)\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find duplicates\n",
    "## https://www.kaggle.com/c/tensorflow-speech-recognition-challenge/discussion/44687"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_fns = sorted(glob('data/train/audio/*/*.wav'))\n",
    "test_fns = sorted(glob('data/test/audio/*.wav'))\n",
    "all_fns = train_fns + test_fns\n",
    "print(\"%d train, %d test, %d total\"\n",
    "      % (len(train_fns), len(test_fns), len(all_fns)))\n",
    "all_fns = all_fns[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('md5sums.p'):\n",
    "    md5sums = {}\n",
    "    for fn in tqdm(all_fns):\n",
    "        if fn in md5sums:\n",
    "            print(\"Double fn!\", fn)\n",
    "        checksum = md5(fn)\n",
    "        md5sums[fn] = checksum\n",
    "    # cache\n",
    "    data = {'md5sums': md5sums}\n",
    "    with open('md5sums.p', 'wb') as f:\n",
    "        pickle.dump(data, f)\n",
    "\n",
    "with open('md5sums.p', 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "    md5sums = data['md5sums']\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = {}\n",
    "checksum_fn = {v: k for k, v in md5sums.items()}\n",
    "for fn, checksum in md5sums.items():\n",
    "    if checksum in counts:\n",
    "        counts[checksum] += 1\n",
    "    else:\n",
    "        counts[checksum] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn_counts = []\n",
    "check_to_fns = {}\n",
    "for checksum, count in counts.items():\n",
    "    fn = checksum_fn[checksum]\n",
    "    fn_counts.append((fn, count))\n",
    "    if checksum in check_to_fns:\n",
    "        if fn not in check_to_fns[fn]:\n",
    "            check_to_fns[fn].append(fn)\n",
    "        else:\n",
    "            check_to_fns[fn] = [fn]\n",
    "fn_counts = sorted(fn_counts, key=lambda x: x[-1], reverse=True)\n",
    "print(fn_counts[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [(k, v) for k, v in counts.items() if v == 3710][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mist_fns = [fn for fn, s in md5sums.items() if s == a[0]]\n",
    "bns = [os.path.basename(fn) for fn in mist_fns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are all silence -.- check that we got it right anyway ...\n",
    "some_sub = pd.read_csv('submission_017.csv')\n",
    "for i in range(some_sub.shape[0]):\n",
    "    fn = some_sub.loc[i, 'fname']\n",
    "    if fn in bns:\n",
    "        label = some_sub.loc[i, 'label']\n",
    "        if label != 'silence':\n",
    "            print(label)\n",
    "# check! That was useless -.-"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Listen test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_fns = sorted(glob('data/test/audio/*.wav'))\n",
    "print(len(test_fns))\n",
    "disp_count = 0\n",
    "for test_fn in test_fns:\n",
    "    if disp_count > 10:\n",
    "        break\n",
    "    if np.random.rand() > 0.7:\n",
    "        print(test_fn)\n",
    "        display(Audio(filename=test_fn,\n",
    "                      rate=16000,\n",
    "                      autoplay=True))\n",
    "        sleep(1)\n",
    "        disp_count += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore subsampled representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_fns = sorted(glob('data/test/audio/*.wav'))\n",
    "some_fn = np.random.choice(test_fns)\n",
    "rate, data = wf.read(some_fn)\n",
    "data = np.float32(data) / 32768\n",
    "assert rate == 16000\n",
    "plt.figure()\n",
    "plt.plot(range(len(data)), data, 'g')\n",
    "display(Audio(some_fn, autoplay=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "step = 10\n",
    "colors = ['r', 'g', 'b', 'y', 'b']\n",
    "for i in range(3):\n",
    "    data_slice = data[0::step]\n",
    "    data_slice = np.roll(data_slice, int(-i * 400))\n",
    "    plt.plot(range(len(data_slice)), data_slice, colors[i])\n",
    "    float_audio(data_slice, sample_rate=int(16000 / step), autoplay=True)\n",
    "    sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(some_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "colors = ['r', 'g', 'b', 'y']\n",
    "step = 3\n",
    "for i in range(step):\n",
    "    sub = data[i::step]\n",
    "    plt.plot(range(len(sub)), sub + 2 * i, colors[i % len(colors)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step = 20\n",
    "float_audio(data[::step], sample_rate=int(16000 / step), autoplay=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do the same thing in keras/tf\n",
    "def time_slice_stack(x, step):\n",
    "    x_slices = []\n",
    "    for i in range(step):\n",
    "        x_slice = x[:, i::step]\n",
    "        x_slice = K.expand_dims(x_slice, axis=-1)\n",
    "        x_slices.append(x_slice)\n",
    "    x_slices = K.concatenate(x_slices, axis=-1)\n",
    "    print(x_slices.shape)\n",
    "    return x_slices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_layer = Input(shape=[16000])\n",
    "x = input_layer\n",
    "x = Lambda(lambda x: time_slice_stack(x, 2))(x)\n",
    "model = Model(input_layer, x)\n",
    "out = model.predict(data.reshape((1, 16000))).squeeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check input representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def plot_show_samples(label='stop'):\n",
    "    fns = sorted(glob('data/train/audio/%s/*.wav' % label))\n",
    "    disp_count = 0\n",
    "    for fn in fns:\n",
    "        if disp_count > 3:\n",
    "            break\n",
    "        rate, data = wf.read(fn)\n",
    "        data = np.float32(data) / 32768\n",
    "        data = pad_crop(data)\n",
    "        print(fn)\n",
    "        plt.figure()\n",
    "        plt.imshow(data.reshape((400, 40)).T)\n",
    "        plt.show()\n",
    "        display(float_audio(data, autoplay=True))\n",
    "        sleep(1)\n",
    "        disp_count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_show_samples()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now check mfcc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "desired_samples = 16000\n",
    "window_size_samples = 480\n",
    "window_stride_samples = 160\n",
    "dct_coefficient_count = 40\n",
    "magnitude_squared = False\n",
    "wav_filename_placeholder = tf.placeholder(tf.string, [])\n",
    "wav_loader = io_ops.read_file(wav_filename_placeholder)\n",
    "wav_decoder = contrib_audio.decode_wav(\n",
    "  wav_loader, desired_channels=1,\n",
    "  desired_samples=desired_samples)\n",
    "clamped = tf.clip_by_value(wav_decoder.audio, -1.0, 1.0)\n",
    "spectrogram = contrib_audio.audio_spectrogram(\n",
    "  clamped,\n",
    "  window_size=window_size_samples,\n",
    "  stride=window_stride_samples,\n",
    "  magnitude_squared=magnitude_squared)\n",
    "mfcc = contrib_audio.mfcc(\n",
    "  spectrogram,\n",
    "  wav_decoder.sample_rate,\n",
    "  dct_coefficient_count=dct_coefficient_count)\n",
    "\n",
    "def load_spectrogram(fn):\n",
    "    with tf.Session() as sess:\n",
    "        spectrogram_val = sess.run(\n",
    "            spectrogram, {wav_filename_placeholder: fn})\n",
    "    return spectrogram_val\n",
    "\n",
    "def load_mfcc(fn):\n",
    "    with tf.Session() as sess:\n",
    "        mfcc_val = sess.run(\n",
    "            mfcc, {wav_filename_placeholder: fn})\n",
    "    return mfcc_val\n",
    "\n",
    "def plot_show(label='stop', output='spec', max_disp=3):\n",
    "    fns = sorted(glob('data/train/audio/%s/*.wav' % label))\n",
    "    disp_count = 0\n",
    "    some_data = None\n",
    "    for fn in fns:\n",
    "        if disp_count >= max_disp:\n",
    "            break\n",
    "        if np.random.rand() > 0.9:\n",
    "            if output == 'spec':\n",
    "                spec = load_spectrogram(fn).squeeze()\n",
    "                some_data = spec\n",
    "            elif output == 'mfcc':\n",
    "                mfcc = load_mfcc(fn).squeeze()\n",
    "                some_data = mfcc\n",
    "            plt.figure()\n",
    "            plt.title(fn)\n",
    "            plt.imshow(some_data.T)\n",
    "            plt.colorbar()\n",
    "            plt.show()\n",
    "            disp_count += 1\n",
    "    return some_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "some_spec = plot_show('sheila', output='spec')\n",
    "print(some_spec.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "some_mfcc = plot_show('stop', output='mfcc', max_disp=3)\n",
    "print(some_mfcc.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Time steps: ', (16000 - 480) / 160 + 1)\n",
    "print('Frequencies: ', 16000 / 160)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "??contrib_audio.audio_spectrogram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Can we create a model that does the same? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_filter = 257\n",
    "\n",
    "input_layer = Input(shape=[16000])\n",
    "x = input_layer\n",
    "x = Reshape([16000, 1])(x)\n",
    "x = Conv1D(num_filter, window_size_samples, strides=window_stride_samples)(x)\n",
    "model = Model(input_layer, x)\n",
    "\n",
    "def plot_show(model, label='stop'):\n",
    "    fns = sorted(glob('data/train/audio/%s/*.wav' % label))\n",
    "    disp_count = 0\n",
    "    some_spec = None\n",
    "    for fn in fns:\n",
    "        if disp_count > 2:\n",
    "            break\n",
    "        if np.random.rand() > 0.0:\n",
    "            rate, data = wf.read(fn)\n",
    "            data = np.float32(data) / 32768\n",
    "            data = pad_crop(data)\n",
    "            spec = model.predict(data.reshape((1, -1))).squeeze()\n",
    "            some_spec = spec\n",
    "            print(spec.shape)\n",
    "            plt.figure()\n",
    "            plt.title(fn)\n",
    "            plt.imshow(spec.T)\n",
    "            plt.show()\n",
    "            disp_count += 1\n",
    "    return some_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "_ = plot_show(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How about a real trained network? This one gets 94% train accuracy and 89% mean validation accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model('checkpoints_069/ep-060-vl-0.3702.hdf5',\n",
    "                 custom_objects={'relu6': relu6,\n",
    "                                 'DepthwiseConv2D': DepthwiseConv2D})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Concatenate\n",
    "concat_layers = [l for l in model.layers if l.__class__ == Concatenate]\n",
    "print(concat_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activation = model.layers[3].output\n",
    "# activation = concat_layers[0].output\n",
    "spec_model = Model(model.input, activation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def norm(x):\n",
    "    return (x - x.mean()) / x.std()\n",
    "\n",
    "fns = sorted(glob('data/train/audio/go/*.wav'))\n",
    "np.random.shuffle(fns)\n",
    "disp_count = 0\n",
    "for fn in fns:\n",
    "    if disp_count > 2:\n",
    "        break\n",
    "    rate, data = wf.read(fn)\n",
    "    data = np.float32(data) / 32768\n",
    "    data = pad_crop(data)\n",
    "    representation = spec_model.predict(data.reshape((1, -1))).squeeze()\n",
    "    spec = load_spectrogram(fn).squeeze()\n",
    "    spec, representation = norm(spec), norm(representation)\n",
    "    print(spec.shape)\n",
    "    plt.figure()\n",
    "    plt.title(fn)\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.imshow(representation.T)\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.imshow(spec.T)\n",
    "    plt.show()\n",
    "    print(representation.shape)\n",
    "    disp_count += 1\n",
    "print(activation.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Looks like we succesfully decorrelated the channels. Though, it seems like this is not what we really want? There shoud be some relation between groups at least -> Use group convolutions? (g-Sub-seperable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# same thing with g-Sub-seperable\n",
    "model = load_model('checkpoints_071/ep-015-vl-0.4296.hdf5',\n",
    "                 custom_objects={'relu6': relu6,\n",
    "                                 'DepthwiseConv2D': DepthwiseConv2D})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Concatenate\n",
    "concat_layers = [l for l in model.layers if l.__class__ == Concatenate]\n",
    "print(concat_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activation = model.layers[3].output\n",
    "# activation = concat_layers[0].output\n",
    "spec_model = Model(model.input, activation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def norm(x):\n",
    "    return (x - x.mean()) / x.std()\n",
    "\n",
    "fns = sorted(glob('data/train/audio/stop/*.wav'))\n",
    "np.random.shuffle(fns)\n",
    "disp_count = 0\n",
    "for fn in fns:\n",
    "    if disp_count > 2:\n",
    "        break\n",
    "    rate, data = wf.read(fn)\n",
    "    data = np.float32(data) / 32768\n",
    "    data = pad_crop(data)\n",
    "    representation = spec_model.predict(data.reshape((1, -1))).squeeze()\n",
    "    spec = load_spectrogram(fn).squeeze()\n",
    "    spec, representation = norm(spec), norm(representation)\n",
    "    print(spec.shape)\n",
    "    plt.figure()\n",
    "    plt.title(fn)\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.imshow(np.flipud(representation.T))\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.imshow(spec.T)\n",
    "    plt.show()\n",
    "    print(representation.shape)\n",
    "    disp_count += 1\n",
    "# print(activation.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What representation is actually learned?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'checkpoints_069/ep-073-vl-0.3997.hdf5'\n",
    "model = my_load_model('checkpoints_069/ep-073-vl-0.3997.hdf5')\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_idx = 3  # 4\n",
    "first_layer_weights = model.layers[layer_idx].get_weights()[0].squeeze()\n",
    "first_layer_biases = model.layers[layer_idx].get_weights()[1]\n",
    "# first_layer_weights = model.layers[layer_idx].get_weights()[0].squeeze()\n",
    "# first_layer_weights = first_layer_weights.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(first_layer_weights.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for w in first_layer_weights:\n",
    "    plt.figure()\n",
    "    plt.plot(range(len(w)), w)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.imshow(first_layer_weights.T, cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = np.dot(first_layer_weights.T, first_layer_weights)\n",
    "plt.imshow(corr, cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Same for the reshape architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = my_load_model('checkpoints_084/ep-087-vl-0.2358.hdf5')\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_idx = 4\n",
    "first_layer_weights = model.layers[layer_idx].get_weights()[0].squeeze()\n",
    "print(first_layer_weights.shape)\n",
    "plt.figure()\n",
    "plt.imshow(first_layer_weights.T, cmap='gray')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_label = 'right'\n",
    "test_fns = sorted(glob('data/train/audio/%s/*.wav' % example_label))\n",
    "int2label = get_int2label(wanted_only=False)\n",
    "num_correct = 0\n",
    "num_total = len(test_fns)\n",
    "vis = True\n",
    "pbar = tqdm(test_fns) if not vis else test_fns\n",
    "for fn in pbar:\n",
    "    rate, data = wf.read(fn)\n",
    "    data = pad_crop(data)\n",
    "    data = np.float32(data) / 32767\n",
    "    prediction = model.predict(data.reshape(1, -1)).squeeze()\n",
    "    label = int2label[prediction.argmax()]\n",
    "    if label != example_label and vis:\n",
    "        print(\"Pred: \", label)\n",
    "        display(Audio(fn, autoplay=True))\n",
    "        sleep(1)\n",
    "    \n",
    "    if label == example_label:\n",
    "        num_correct += 1\n",
    "print(\"Acc: %.3f\" % (float(num_correct) / num_total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activation_model = Model(model.input, model.layers[40].output)\n",
    "print(activation_model.output_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "test_fns = sorted(glob('data/train/audio/stop/*.wav'))\n",
    "int2label = get_int2label(wanted_only=False)\n",
    "for i in range(2):\n",
    "    for fn in [test_fns[np.random.randint(len(test_fns))]]:\n",
    "        print(fn)\n",
    "        rate, data = wf.read(fn)\n",
    "        data = pad_crop(data)\n",
    "        data = np.float32(data) / 32767\n",
    "        act_val = activation_model.predict(data.reshape(1, -1)).squeeze()\n",
    "        print(act_val.min(), act_val.max())\n",
    "        display(Audio(fn, autoplay=True))\n",
    "        plt.figure()\n",
    "        plt.subplot(4, 1, 1)\n",
    "        plt.imshow(cv2.resize(act_val.T, (16000, act_val.shape[0] * 20), interpolation=cv2.INTER_NEAREST))\n",
    "        plt.subplot(4, 1, 2)\n",
    "        plt.plot(data)\n",
    "        plt.subplot(4, 1, 3)\n",
    "        corr_mat = np.dot(act_val, act_val.T)\n",
    "        print(corr_mat.shape)\n",
    "        plt.imshow(cv2.resize(corr_mat, (16000, corr_mat.shape[0] * 20), interpolation=cv2.INTER_NEAREST))\n",
    "        plt.colorbar()\n",
    "        plt.subplot(4, 1, 4)\n",
    "        plt.plot(data[::40])\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.arange(16000)\n",
    "print(a.reshape((400, 40)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# There are a few wrong labels in the training data. Can we find them using ML?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load best single model\n",
    "model = my_load_model('checkpoints_086/ep-110-vl-0.1935.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_fns = sorted(glob('data/train/audio/*/*.wav'))\n",
    "train_fns = [fn for fn in train_fns\n",
    "             if fn.split('/')[-2] != '_background_noise_' and\n",
    "             fn.split('/')[-2] != 'unknown_unknown']\n",
    "print(\"Found: %d fns\" % len(train_fns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "int2label = get_int2label(wanted_only=False)\n",
    "fns, preds, gts = [], [], []\n",
    "for fn in tqdm(train_fns[:]):\n",
    "    rate, data = wf.read(fn)\n",
    "    if len(data) != 16000:\n",
    "        data = pad_crop(data)\n",
    "    data = np.float32(data) / 32768\n",
    "    pred = model.predict(data.reshape((1, -1))).squeeze()\n",
    "    gt_label = fn.split('/')[-2]\n",
    "    pred_label = int2label[pred.argmax()]\n",
    "    if pred_label != gt_label:\n",
    "        fns.append(fn)\n",
    "        preds.append(pred_label)\n",
    "        gts.append(gt_label)\n",
    "\n",
    "pd.DataFrame({'filename': fns,\n",
    "              'predicted_label': preds,\n",
    "              'gt_label': gts}).to_csv('wrong_train_labels.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wc -l wrong_train_labels.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrong_labels = pd.read_csv('wrong_train_labels.csv')\n",
    "print(wrong_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(wrong_labels.shape[0]):\n",
    "    fn = wrong_labels.loc[i, 'filename']\n",
    "    gt = wrong_labels.loc[i, 'gt_label']\n",
    "    pred = wrong_labels.loc[i, 'predicted_label']\n",
    "    if gt == 'stop':\n",
    "        print(\"%s: GT: %s, PRED: %s\" % (fn, gt, pred))\n",
    "        display(Audio(fn, autoplay=True))\n",
    "        sleep(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Can we speed down the command? This is how I'd repeat something if a person doesn't unterstand me :P (probably louder as well)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_fns = sorted(glob('data/train/audio/*/*.wav'))\n",
    "print(len(train_fns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(13)\n",
    "fn = np.random.choice(train_fns)\n",
    "print(fn)\n",
    "rate, data = wf.read(fn)\n",
    "if len(data) != 16000:\n",
    "    data = pad_crop(data)\n",
    "data = np.float32(data) / 32767\n",
    "float_audio(data, autoplay=True)\n",
    "print(data.shape)\n",
    "plt.figure()\n",
    "plt.plot(data)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cv2? lol: https://www.kaggle.com/haqishen/augmentation-methods-for-audio\n",
    "# The pitch chanes (sounds bad!)\n",
    "speed_rate = 0.8\n",
    "w = int(len(data) / speed_rate)\n",
    "print(w)\n",
    "data_fast = data.reshape((1, -1))\n",
    "data_fast = cv2.resize(\n",
    "    data_fast, (w, 1)).squeeze()\n",
    "data_fast = data_fast[4000: 20000]\n",
    "float_audio(data_fast, autoplay=True)\n",
    "plt.figure()\n",
    "plt.plot(data_fast)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try the same with librosa\n",
    "### That sounds good!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def center_crop(data, desired_size=16000):\n",
    "    left = (len(data) - desired_size) // 2\n",
    "    return data[left: left + desired_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "speed = 1.3  # float: < 1.0: slow down, > 1.0: speed up\n",
    "data_fast = lr.effects.time_stretch(data, speed)\n",
    "print(data_fast.shape)\n",
    "# data_fast = data_fast[4000: 20000]\n",
    "float_audio(data_fast, autoplay=True)\n",
    "plt.figure()\n",
    "plt.plot(data_fast)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check the attention model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 16000)        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_2 (Lambda)               (None, 16000)        0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_3 (Lambda)               (None, 800, 40)      0           lambda_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 399, 64)      7680        lambda_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 399, 64)      256         conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 399, 64)      0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "lambda_4 (Lambda)               (None, 1, 399, 64)   0           activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "depthwise_conv2d_1 (DepthwiseCo (None, 1, 397, 64)   192         lambda_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_5 (Lambda)               (None, 397, 64)      0           depthwise_conv2d_1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 397, 128)     8192        lambda_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 397, 128)     512         conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 397, 128)     0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "lambda_6 (Lambda)               (None, 1, 397, 128)  0           activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "depthwise_conv2d_2 (DepthwiseCo (None, 1, 199, 128)  384         lambda_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_7 (Lambda)               (None, 199, 128)     0           depthwise_conv2d_2[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, 199, 192)     24576       lambda_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 199, 192)     768         conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 199, 192)     0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "lambda_8 (Lambda)               (None, 1, 199, 192)  0           activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "depthwise_conv2d_3 (DepthwiseCo (None, 1, 197, 192)  576         lambda_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_9 (Lambda)               (None, 197, 192)     0           depthwise_conv2d_3[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_4 (Conv1D)               (None, 197, 192)     36864       lambda_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 197, 192)     768         conv1d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 197, 192)     0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "lambda_10 (Lambda)              (None, 1, 197, 192)  0           activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "depthwise_conv2d_4 (DepthwiseCo (None, 1, 99, 192)   576         lambda_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_11 (Lambda)              (None, 99, 192)      0           depthwise_conv2d_4[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_5 (Conv1D)               (None, 99, 256)      49152       lambda_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 99, 256)      1024        conv1d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 99, 256)      0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "lambda_12 (Lambda)              (None, 1, 99, 256)   0           activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "depthwise_conv2d_5 (DepthwiseCo (None, 1, 97, 256)   768         lambda_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_13 (Lambda)              (None, 97, 256)      0           depthwise_conv2d_5[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_6 (Conv1D)               (None, 97, 256)      65536       lambda_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 97, 256)      1024        conv1d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 97, 256)      0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "lambda_14 (Lambda)              (None, 1, 97, 256)   0           activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "depthwise_conv2d_6 (DepthwiseCo (None, 1, 49, 256)   768         lambda_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_15 (Lambda)              (None, 49, 256)      0           depthwise_conv2d_6[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_7 (Conv1D)               (None, 49, 320)      81920       lambda_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 49, 320)      1280        conv1d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 49, 320)      0           batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "lambda_16 (Lambda)              (None, 1, 49, 320)   0           activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "depthwise_conv2d_7 (DepthwiseCo (None, 1, 47, 320)   960         lambda_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_17 (Lambda)              (None, 47, 320)      0           depthwise_conv2d_7[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_8 (Conv1D)               (None, 47, 320)      102400      lambda_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 47, 320)      1280        conv1d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 47, 320)      0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "lambda_18 (Lambda)              (None, 1, 47, 320)   0           activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "depthwise_conv2d_8 (DepthwiseCo (None, 1, 24, 320)   960         lambda_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_19 (Lambda)              (None, 24, 320)      0           depthwise_conv2d_8[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_9 (Conv1D)               (None, 24, 384)      122880      lambda_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 24, 384)      1536        conv1d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 24, 384)      0           batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "lambda_20 (Lambda)              (None, 1, 24, 384)   0           activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "depthwise_conv2d_9 (DepthwiseCo (None, 1, 22, 384)   1152        lambda_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_21 (Lambda)              (None, 22, 384)      0           depthwise_conv2d_9[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_10 (Conv1D)              (None, 22, 384)      147456      lambda_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 22, 384)      1536        conv1d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 22, 384)      0           batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "lambda_22 (Lambda)              (None, 1, 22, 384)   0           activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "depthwise_conv2d_10 (DepthwiseC (None, 1, 11, 384)   1152        lambda_22[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_23 (Lambda)              (None, 11, 384)      0           depthwise_conv2d_10[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_11 (Conv1D)              (None, 11, 448)      172032      lambda_23[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 11, 448)      1792        conv1d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 11, 448)      0           batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "lambda_24 (Lambda)              (None, 1, 11, 448)   0           activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "depthwise_conv2d_11 (DepthwiseC (None, 1, 9, 448)    1344        lambda_24[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_25 (Lambda)              (None, 9, 448)       0           depthwise_conv2d_11[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_12 (Conv1D)              (None, 9, 448)       200704      lambda_25[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 9, 448)       1792        conv1d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 9, 448)       0           batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 9, 448)       0           activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 4032)         0           dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 9)            36288       flatten_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_26 (Lambda)              (None, 9, 1)         0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "multiply_1 (Multiply)           (None, 9, 448)       0           activation_12[0][0]              \n",
      "                                                                 lambda_26[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_1 (Glo (None, 448)          0           multiply_1[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 448)          0           global_average_pooling1d_1[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 32)           14336       dropout_2[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 1,092,416\n",
      "Trainable params: 1,085,632\n",
      "Non-trainable params: 6,784\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = my_load_model('checkpoints_109/ep-073-vl-0.1745.hdf5')\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 9, 1)\n"
     ]
    }
   ],
   "source": [
    "attention = Model(model.input, model.layers[-5].output)\n",
    "print(attention.output_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_fns = sorted(glob('data/train/audio/*/*.wav'))\n",
    "np.random.shuffle(test_fns)\n",
    "disp_count = 0\n",
    "for fn in test_fns:\n",
    "    if disp_count > 3:\n",
    "        break\n",
    "    print(fn)\n",
    "    rate, data = wf.read(fn)\n",
    "    data = np.float32(data) / 32767\n",
    "    data = pad_crop(data)\n",
    "    attention_val = attention.predict(data.reshape((1, -1))).reshape((1, -1))\n",
    "    attention_val = cv2.resize(attention_val, (16000, 1)).squeeze()\n",
    "    disp_count += 1\n",
    "    # print(attention_val)\n",
    "    plt.subplot(2, 1, 1)\n",
    "    plt.plot(range(len(attention_val)), attention_val)\n",
    "    plt.axis([0, len(attention_val), 0, 1])\n",
    "    plt.subplot(2, 1, 2)\n",
    "    plt.plot(range(len(data)), data)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYwAAAD8CAYAAABkbJM/AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ndqaXnxMAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7efefc06f518>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = np.roll(data, -3000)\n",
    "attention_val = attention.predict(data.reshape((1, -1))).reshape((1, -1))\n",
    "attention_val = cv2.resize(attention_val, (16000, 1)).squeeze()\n",
    "disp_count += 1\n",
    "# print(attention_val)\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(range(len(attention_val)), attention_val)\n",
    "plt.axis([0, len(attention_val), 0, 1])\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(range(len(data)), data)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
